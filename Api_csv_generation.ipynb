{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests, json, os, csv, re, urllib.parse\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from time import sleep\n",
    "from pathlib import Path\n",
    "from contextlib import ExitStack\n",
    "from datetime import datetime\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.environ.get('API_KEY')\n",
    "headers = {\"x-api-key\": api_key}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_id_valid(paper):\n",
    "    return paper is not None and paper['paperId'] is not None\n",
    "\n",
    "def is_valid_paper(paper):\n",
    "    return paper['authors'] and paper['abstract'] is not None and paper['title'] is not None and paper['year'] is not None and paper['publicationTypes'] is not None\n",
    "\n",
    "\n",
    "def is_valid_conference(paper):\n",
    "    return \"Conference\" in paper[\"publicationTypes\"] and paper[\"venue\"] is not None and paper[\"publicationVenue\"] is not None\n",
    "\n",
    "\n",
    "def is_valid_journal(paper):\n",
    "    return \"JournalArticle\" in paper[\"publicationTypes\"] and paper[\"publicationVenue\"] is not None and paper[\"journal\"] is not None and \"name\" in paper[\"journal\"] and \"pages\" in paper[\"journal\"] and \"volume\" in paper[\"journal\"]\n",
    "\n",
    "\n",
    "def get_referencing_author_id(authors):\n",
    "    return authors[0]['authorId']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV Needed\n",
    "- paper (w abstract and relevant author) \n",
    "- paper-paper (n-n)\n",
    "- author \n",
    "- paper-author (n-n)\n",
    "- paper-reviewers (n-n)\n",
    "- keywords\n",
    "- paper-keywords (n-n)\n",
    "- conference (1-n)\n",
    "- journal (1-n)\n",
    "- year (1-n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = {\n",
    "    \"paper\": [\"paperId\",\"corpusId\", \"title\", \"referenceAuthorId\", \"abstract\", \"url\", \"publicationType\", \"publicationDate\",\"journalId\", \"conferenceId\",\"yearId\"],\n",
    "    \"paper_paper\": [\"citingPaperId\", \"citedPaperId\"],\n",
    "    \"author\": [\"authorId\", \"authorName\"],\n",
    "    \"paper_author\": [\"paperId\", \"authorId\"],\n",
    "    \"paper_reviewer\": [\"paperId\", \"reviewAuthorId\"],\n",
    "    \"keywords\": [\"keyword\"],\n",
    "    \"paper_keywords\": [\"paperId\", \"keyword\"],\n",
    "    \"conference\": [\"conferenceId\", \"conferenceName\", \"yearId\"],\n",
    "    \"journal\": [\"journalId\", \"journalName\", \"journalPages\", \"journalVolume\",\"yearId\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'years.csv' created successfully!\n"
     ]
    }
   ],
   "source": [
    "#This is synthetic (Not worthy to generate it)\n",
    "years = list(range(1950, datetime.now().year + 1))\n",
    "ids = [year-1950 for year in years]\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\"ids\":ids, \"Year\": years})\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"csv/years.csv\", index=False)\n",
    "\n",
    "print(\"CSV file 'years.csv' created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#********************************************************************************************************************\n",
    "RECORDS = 100  # Number of records to save per category \n",
    "QUERY = \"semantic data modelling and property graphs\"  # Query to filter the papers\n",
    "FIELDS = \"paperId,corpusId,title,abstract,authors,url,year,s2FieldsOfStudy,publicationDate,publicationTypes,journal,venue,publicationVenue,references.paperId\"  # Fields to retrieve from the API\n",
    "#********************************************************************************************************************\n",
    "\n",
    "query_encoded = urllib.parse.quote(QUERY)\n",
    "fields_encoded = urllib.parse.quote(FIELDS)\n",
    "type_encoded = urllib.parse.quote(\"Conference,JournalArticle\")\n",
    "\n",
    "starting_papers_url=\"https://api.semanticscholar.org/graph/v1/paper/search?query=\"+query_encoded+\"&publicationTypes=\"+type_encoded+\"&fields=paperId&limit=\"+str(RECORDS)\n",
    "response = requests.get(starting_papers_url, headers=headers).json()\n",
    "starting_papers = response[\"data\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_new_papers(processed_papers, to_be_processed_papers, processing_papers, new_papers):\n",
    "    new_papers.discard(None)\n",
    "    for paper in new_papers:\n",
    "        if paper not in processed_papers and paper not in to_be_processed_papers and paper not in processing_papers:\n",
    "            to_be_processed_papers.add(paper)\n",
    "\n",
    "\n",
    "def choose_n_papers_to_process(to_be_processed_papers, n):\n",
    "    return {to_be_processed_papers.pop() for _ in range(min(n, len(to_be_processed_papers)))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 200\n",
    "MAX_RECURSION = 10\n",
    "csv_folder = Path('csv')\n",
    "\n",
    "processed_papers = set()\n",
    "to_be_processed_papers = set()\n",
    "starting_papers_ids = set([paper['paperId'] for paper in starting_papers])\n",
    "process_new_papers(processed_papers, to_be_processed_papers, set(), starting_papers_ids)\n",
    "\n",
    "set_authors = set()\n",
    "set_keywords = set()\n",
    "set_papers = set()\n",
    "set_joutnals = set()\n",
    "set_conferences = set()\n",
    "\n",
    "with ExitStack() as stack:  # Ensures all files are closed properly\n",
    "    files = {name: stack.enter_context(open(csv_folder / (name + '.csv'), \"w\", newline='', encoding=\"utf-8\")) for name in csv_files}\n",
    "    writers = {name: csv.DictWriter(files[name], fieldnames=fieldnames, delimiter=\"|\") for name, fieldnames in csv_files.items()}\n",
    "    recursion_block = 0\n",
    "\n",
    "    # Header has to be removed for tables and changed for relationships! Ask Alfio\n",
    "    for writer in writers.values():\n",
    "        writer.writeheader()\n",
    "\n",
    "    while to_be_processed_papers:\n",
    "        recursion_block+=1\n",
    "        if recursion_block > MAX_RECURSION:\n",
    "            break\n",
    "\n",
    "        processing_papers_id = choose_n_papers_to_process(to_be_processed_papers, BATCH_SIZE)\n",
    "\n",
    "        processing_papers_data = requests.post(\n",
    "            'https://api.semanticscholar.org/graph/v1/paper/batch',\n",
    "            params={'fields': FIELDS},\n",
    "            json={\"ids\": list(processing_papers_id)},\n",
    "            headers=headers\n",
    "        ).json()\n",
    "        \n",
    "        for paper in processing_papers_data:\n",
    "            try:  \n",
    "                if not is_id_valid(paper):\n",
    "                    continue   \n",
    "                processed_papers.add(paper['paperId'])\n",
    "                if not is_valid_paper(paper):\n",
    "                    continue\n",
    "                if is_valid_conference(paper):\n",
    "                    paper[\"publicationType\"]=\"Conference\"\n",
    "                    paper[\"journalName\"]=None\n",
    "                    paper[\"journalVolume\"]=None\n",
    "                    paper[\"journalPages\"]=None\n",
    "                    paper[\"conferenceId\"]=paper[\"publicationVenue\"][\"id\"]\n",
    "                    paper[\"journalId\"]=None\n",
    "                    if paper[\"conferenceId\"]+str(paper[\"year\"]) not in set_conferences:\n",
    "                        set_conferences.add(paper[\"conferenceId\"]+str(paper[\"year\"]))\n",
    "                        writers['conference'].writerow({\n",
    "                        \"conferenceId\": paper[\"conferenceId\"],\n",
    "                        \"conferenceName\": paper[\"publicationVenue\"][\"name\"],\n",
    "                        \"yearId\": paper[\"year\"]-1950,\n",
    "                        })\n",
    "                    \n",
    "                    \n",
    "                elif is_valid_journal(paper):\n",
    "                    paper[\"publicationType\"]=\"JournalArticle\"\n",
    "                    paper[\"journalId\"]=paper[\"publicationVenue\"][\"id\"]\n",
    "                    paper[\"venue\"]=None\n",
    "                    paper[\"conferenceId\"]=None\n",
    "                    paper[\"journalName\"]=paper[\"journal\"][\"name\"]\n",
    "                    paper[\"journalVolume\"]=paper[\"journal\"][\"volume\"]\n",
    "                    if paper[\"journal\"][\"pages\"] is not None:\n",
    "                        paper[\"journalPages\"]=re.sub(r'\\s+', '', paper[\"journal\"][\"pages\"])\n",
    "                    else:\n",
    "                        paper[\"journalPages\"]=None\n",
    "                    if paper[\"journalId\"]+str(paper[\"year\"]) not in set_joutnals:\n",
    "                        set_joutnals.add(paper[\"journalId\"]+str(paper[\"year\"]))\n",
    "                        writers['journal'].writerow({\n",
    "                        \"journalId\": paper[\"journalId\"],\n",
    "                        \"journalName\": paper[\"journalName\"],\n",
    "                        \"journalVolume\": paper[\"journalVolume\"],\n",
    "                        \"journalPages\": paper[\"journalPages\"], \n",
    "                        \"yearId\": paper[\"year\"]-1950,\n",
    "                        })\n",
    "                    \n",
    "                else:\n",
    "                    continue   \n",
    "                paperId = paper.get(\"paperId\")\n",
    "                paper_authors = paper[\"authors\"]\n",
    "                writers['paper'].writerow({\n",
    "                    \"paperId\": paperId,\n",
    "                    \"corpusId\": paper.get(\"corpusId\"),\n",
    "                    \"title\":  paper.get(\"title\").strip().replace(\"\\n\", \" \").replace(\"|\", \" \").replace('\"', \"\").replace(\"^\", \" \"),\n",
    "                    \"referenceAuthorId\": get_referencing_author_id(paper_authors),\n",
    "                    \"abstract\": paper.get(\"abstract\").strip().replace(\"\\n\", \" \").replace(\"|\", \" \").replace('\"', \"\").replace(\"^\", \" \"),\n",
    "                    \"url\": paper.get(\"url\"),\n",
    "                    \"yearId\": paper.get(\"year\")-1950,\n",
    "                    \"publicationType\": paper.get(\"publicationType\"),\n",
    "                    \"publicationDate\": paper.get(\"publicationDate\"),\n",
    "                    \"journalId\": paper.get(\"journalId\"),\n",
    "                    \"conferenceId\": paper.get(\"conferenceId\")\n",
    "                })\n",
    "\n",
    "                new_papers = set([paper['paperId'] for paper in paper['references']])\n",
    "                process_new_papers(processed_papers, to_be_processed_papers, processing_papers_id, new_papers)\n",
    "\n",
    "                for new_paper in new_papers:\n",
    "                    writers['paper_paper'].writerow({\n",
    "                        \"citingPaperId\": paperId,\n",
    "                        \"citedPaperId\": new_paper,\n",
    "                    })\n",
    "\n",
    "                for author in paper_authors:\n",
    "                    authorId = author.get(\"authorId\")\n",
    "                    writers['paper_author'].writerow({\n",
    "                        \"paperId\": paperId,\n",
    "                        \"authorId\": authorId\n",
    "                    })\n",
    "\n",
    "                    if authorId not in set_authors:\n",
    "                        writers['author'].writerow({\n",
    "                            \"authorId\": authorId,\n",
    "                            \"authorName\": author.get(\"name\")\n",
    "                        })\n",
    "\n",
    "                        set_authors.add(authorId)\n",
    "            \n",
    "                paper_keywords = paper.get(\"s2FieldsOfStudy\", [])\n",
    "                paper_keywords = set(map(lambda x: x['category'], paper_keywords))\n",
    "\n",
    "                for keyword in paper_keywords:\n",
    "                    writers[\"paper_keywords\"].writerow({\n",
    "                            \"paperId\": paperId,\n",
    "                            \"keyword\": keyword\n",
    "                        })\n",
    "                    \n",
    "                    if keyword not in set_keywords:\n",
    "                        writers[\"keywords\"].writerow({\n",
    "                            \"keyword\": keyword\n",
    "                        })\n",
    "\n",
    "                        set_keywords.add(keyword)\n",
    "                \n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines in papers: 840\n",
      "Number of lines in events: 504\n"
     ]
    }
   ],
   "source": [
    "#If the numbers are the same means that every paper has a different event, we want to have groups!!\n",
    "import pandas as pd\n",
    "papers_file = \"csv/paper.csv\"\n",
    "conferences_file = \"csv/conference.csv\"\n",
    "journals_file = \"csv/journal.csv\"\n",
    "\n",
    "df=pd.read_csv(papers_file, delimiter='|')\n",
    "df_conferences=pd.read_csv(conferences_file, delimiter='|')\n",
    "df_journals=pd.read_csv(journals_file, delimiter='|')\n",
    "\n",
    "print(\"Number of lines in papers:\", len(df))\n",
    "print(\"Number of lines in events:\", len(df_conferences)+len(df_journals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of references: 34455\n",
      "Number of references after cleaning: 2144\n"
     ]
    }
   ],
   "source": [
    "#Cleaning references :)\n",
    "citations_file=\"csv/paper_paper.csv\"\n",
    "\n",
    "df_citations=pd.read_csv(citations_file, delimiter='|')\n",
    "print(\"Number of references:\", len(df_citations))\n",
    "df_citations_new = df_citations[df_citations[\"citedPaperId\"].isin(df['paperId'].to_list())]\n",
    "print(\"Number of references after cleaning:\", len(df_citations_new))\n",
    "df_citations_new.to_csv(\"csv/paper_paper.csv\", sep='|', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for Knowledge Discovery and Data Mining in 2024: Error code: 404 - {'error': {'message': 'The model `gpt-3` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nopenai.api_key = os.environ.get(\\'GPT_KEY\\')\\n\\n# Function to query GPT for the host country\\ndef get_host_country(conference, year):\\n    prompt = f\"Where was the {conference} conference hosted in {year}?\"\\n    \\n    try:\\n        response = openai.ChatCompletion.create(\\n            model=\"gpt-4\",\\n            messages=[{\"role\": \"user\", \"content\": prompt}]\\n        )\\n        return response[\"choices\"][0][\"message\"][\"content\"]\\n    \\n    except Exception as e:\\n        print(f\"Error fetching data for {conference} in {year}: {e}\")\\n        return None\\n\\n# Iterate through the DataFrame and call the GPT API\\ndf_conferences[\\'Country\\'] = df_conferences.apply(lambda row: get_host_country(row[\"conferenceName\"], row[\\'yearId\\']+1950), axis=1)\\n\\ndf_conferences.to_csv(\"csv/conference.csv\", sep=\\'|\\', index=False)\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import openai\n",
    "\n",
    "openai.api_key = os.environ.get('GPT_KEY')\n",
    "# Sample function to fetch the host country from GPT\n",
    "def get_host_country(conference, year):\n",
    "    prompt = f\"Where was the {conference} conference hosted in {year}?\"\n",
    "    \n",
    "    try:\n",
    "        response = openai.completions.create(\n",
    "            model=\"gpt-3\",\n",
    "            prompt=prompt,\n",
    "            max_tokens=50\n",
    "        )\n",
    "        return response[\"choices\"][0][\"text\"].strip()  # Adjusted field name\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {conference} in {year}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "response = get_host_country(\"Knowledge Discovery and Data Mining\", 2024)\n",
    "print(response)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "openai.api_key = os.environ.get('GPT_KEY')\n",
    "\n",
    "# Function to query GPT for the host country\n",
    "def get_host_country(conference, year):\n",
    "    prompt = f\"Where was the {conference} conference hosted in {year}?\"\n",
    "    \n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {conference} in {year}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Iterate through the DataFrame and call the GPT API\n",
    "df_conferences['Country'] = df_conferences.apply(lambda row: get_host_country(row[\"conferenceName\"], row['yearId']+1950), axis=1)\n",
    "\n",
    "df_conferences.to_csv(\"csv/conference.csv\", sep='|', index=False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified JSONL saved to csv/authors.csv\n"
     ]
    }
   ],
   "source": [
    "url = \"https://api.semanticscholar.org/graph/v1/author/batch\"\n",
    "output_file=\"csv/authors.csv\"\n",
    "papers_file = \"csv/papers.csv\" \n",
    "count=0  \n",
    "query_params = {\n",
    "    \"fields\": \"name,url,paperCount,hIndex\"#,papers\"\n",
    "}\n",
    "\n",
    "df=pd.read_csv(papers_file)\n",
    "ids=df[\"authorId\"].values.tolist()\n",
    "\n",
    "data = {\n",
    "    \"ids\": ids\n",
    "}\n",
    "api_key = os.environ.get('API_KEY')\n",
    "headers = {\"x-api-key\": api_key}\n",
    "\n",
    "# Send the API request\n",
    "response = requests.post(url, params=query_params, json=data, headers=headers).json()\n",
    "# Save the results to json file\n",
    "with open(output_file, \"w\", newline='', encoding=\"utf-8\") as outfile  :   \n",
    "    csv_writer_1 = csv.DictWriter(outfile, fieldnames=[\"sid\",\"authorId\", \"url\", \"name\", \"paperCount\", \"hIndex\"])\n",
    "\n",
    "    # Write the headers to the CSV files\n",
    "    csv_writer_1.writeheader()\n",
    "    for paper in response:\n",
    "        count+=1\n",
    "        try:  \n",
    "            paper_row = {\n",
    "                        \"sid\": count, # Add a new column with a surrogated ID, just in case\n",
    "                        \"authorId\": paper.get(\"authorId\"),\n",
    "                        \"url\": paper.get(\"url\"),\n",
    "                        \"name\": paper.get(\"name\"),\n",
    "                        \"paperCount\": paper.get(\"paperCount\"),\n",
    "                        \"hIndex\": paper.get(\"hIndex\")\n",
    "                        }\n",
    "            # Write the row to CSV 1\n",
    "            csv_writer_1.writerow(paper_row)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON: {e}\")\n",
    "print(f\"Modified JSONL saved to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
