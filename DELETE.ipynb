{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests, json, os, csv, re, urllib.parse\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from time import sleep\n",
    "from pathlib import Path\n",
    "from contextlib import ExitStack\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.environ.get('API_KEY')\n",
    "headers = {\"x-api-key\": api_key}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_id_valid(paper):\n",
    "    return paper is not None and paper['paperId'] is not None\n",
    "\n",
    "def is_valid_paper(paper):\n",
    "    return paper['authors'] and paper['abstract'] is not None and paper['title'] is not None and paper['year'] is not None and paper['publicationTypes'] is not None and paper[\"publicationVenue\"] is not None\n",
    "\n",
    "\n",
    "def is_valid_conference(paper):\n",
    "    return \"Conference\" in paper[\"publicationTypes\"] and paper[\"venue\"] is not None \n",
    "\n",
    "\n",
    "def is_valid_journal(paper):\n",
    "    return \"JournalArticle\" in paper[\"publicationTypes\"] and paper[\"journal\"] is not None and \"name\" in paper[\"journal\"] and \"pages\" in paper[\"journal\"] and \"volume\" in paper[\"journal\"]\n",
    "\n",
    "\n",
    "def get_referencing_author_id(authors):\n",
    "    return authors[0]['authorId']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV Needed\n",
    "- paper (w abstract and relevant author) \n",
    "- paper-paper (n-n)\n",
    "- author \n",
    "- paper-author (n-n)\n",
    "- paper-reviewers (n-n)\n",
    "- keywords\n",
    "- paper-keywords (n-n)\n",
    "- conference (1-n)\n",
    "- journal (1-n)\n",
    "- year (1-n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = {\n",
    "    \"paper\": [\"paperId\",\"corpusId\", \"title\", \"referenceAuthorId\", \"abstract\", \"url\", \"publicationType\", \"publicationDate\",\"publicationId\",\"yearId\"],\n",
    "    \"paper_paper\": [\"citingPaperId\", \"citedPaperId\"],\n",
    "    \"author\": [\"authorId\", \"authorName\"],\n",
    "    \"paper_author\": [\"paperId\", \"authorId\"],\n",
    "    \"paper_reviewer\": [\"paperId\", \"reviewAuthorId\"],\n",
    "    \"keywords\": [\"keyword\"],\n",
    "    \"paper_keywords\": [\"paperId\", \"keyword\"],\n",
    "    \"conference\": [\"conferenceId\", \"conferenceName\", \"yearId\", \"cityId\"],\n",
    "    \"journal\": [\"journalId\", \"journalName\", \"journalPages\", \"journalVolume\",\"yearId\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is synthetic (Not worthy to generate it)\n",
    "years = list(range(1950, datetime.now().year + 1))\n",
    "ids = [year-1950 for year in years]\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\"ids\":ids, \"Year\": years})\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"csv/years.csv\", sep=\"|\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#********************************************************************************************************************\n",
    "RECORDS = 100  # Number of records to save per category \n",
    "QUERY = \"semantic data modelling and property graphs\"  # Query to filter the papers\n",
    "FIELDS = \"paperId,corpusId,title,abstract,authors,url,year,s2FieldsOfStudy,publicationDate,publicationTypes,journal,venue,publicationVenue,references.paperId\"  # Fields to retrieve from the API\n",
    "#********************************************************************************************************************\n",
    "\n",
    "query_encoded = urllib.parse.quote(QUERY)\n",
    "fields_encoded = urllib.parse.quote(FIELDS)\n",
    "type_encoded = urllib.parse.quote(\"Conference,JournalArticle\")\n",
    "\n",
    "starting_papers_url=\"https://api.semanticscholar.org/graph/v1/paper/search?query=\"+query_encoded+\"&publicationTypes=\"+type_encoded+\"&fields=paperId&limit=\"+str(RECORDS)\n",
    "response = requests.get(starting_papers_url, headers=headers).json()\n",
    "starting_papers = response[\"data\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_new_papers(processed_papers, to_be_processed_papers, processing_papers, new_papers):\n",
    "    new_papers.discard(None)\n",
    "    for paper in new_papers:\n",
    "        if paper not in processed_papers and paper not in to_be_processed_papers and paper not in processing_papers:\n",
    "            to_be_processed_papers.add(paper)\n",
    "\n",
    "\n",
    "def choose_n_papers_to_process(to_be_processed_papers, n):\n",
    "    return {to_be_processed_papers.pop() for _ in range(min(n, len(to_be_processed_papers)))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 200\n",
    "MAX_RECURSION = 10\n",
    "csv_folder = Path('csv')\n",
    "cities = pd.read_csv('csv/city.csv')\n",
    "seed_value = 42\n",
    "np.random.seed(seed_value) \n",
    "\n",
    "processed_papers = set()\n",
    "to_be_processed_papers = set()\n",
    "starting_papers_ids = set([paper['paperId'] for paper in starting_papers])\n",
    "process_new_papers(processed_papers, to_be_processed_papers, set(), starting_papers_ids)\n",
    "\n",
    "set_authors = set()\n",
    "set_keywords = set()\n",
    "set_papers = set()\n",
    "set_joutnals = set()\n",
    "set_conferences = set()\n",
    "\n",
    "with ExitStack() as stack:  # Ensures all files are closed properly\n",
    "    files = {name: stack.enter_context(open(csv_folder / (name + '.csv'), \"w\", newline='', encoding=\"utf-8\")) for name in csv_files}\n",
    "    writers = {name: csv.DictWriter(files[name], fieldnames=fieldnames, delimiter=\"|\") for name, fieldnames in csv_files.items()}\n",
    "    recursion_block = 0\n",
    "\n",
    "    # Header has to be removed for tables and changed for relationships! Ask Alfio\n",
    "    for writer in writers.values():\n",
    "        writer.writeheader()\n",
    "\n",
    "    while to_be_processed_papers:\n",
    "        recursion_block+=1\n",
    "        if recursion_block > MAX_RECURSION:\n",
    "            break\n",
    "\n",
    "        processing_papers_id = choose_n_papers_to_process(to_be_processed_papers, BATCH_SIZE)\n",
    "\n",
    "        processing_papers_data = requests.post(\n",
    "            'https://api.semanticscholar.org/graph/v1/paper/batch',\n",
    "            params={'fields': FIELDS},\n",
    "            json={\"ids\": list(processing_papers_id)},\n",
    "            headers=headers\n",
    "        ).json()\n",
    "        \n",
    "        for paper in processing_papers_data:\n",
    "            try:  \n",
    "                if not is_id_valid(paper):\n",
    "                    continue   \n",
    "                processed_papers.add(paper['paperId'])\n",
    "                if not is_valid_paper(paper):\n",
    "                    continue\n",
    "                paper[\"publicationId\"]=paper[\"publicationVenue\"][\"id\"]\n",
    "                if is_valid_conference(paper):\n",
    "                    paper[\"publicationType\"]=\"Conference\"\n",
    "                    paper[\"journalName\"]=None\n",
    "                    paper[\"journalVolume\"]=None\n",
    "                    paper[\"journalPages\"]=None\n",
    "                    if paper[\"publicationId\"]+str(paper[\"year\"]) not in set_conferences:\n",
    "                        set_conferences.add(paper[\"publicationId\"]+str(paper[\"year\"]))\n",
    "                        writers['conference'].writerow({\n",
    "                        \"conferenceId\": paper[\"publicationId\"],\n",
    "                        \"conferenceName\": paper[\"publicationVenue\"][\"name\"],\n",
    "                        \"yearId\": paper[\"year\"]-1950,\n",
    "                        \"cityId\": cities['id'].sample(n=1, replace=True).iloc[0]\n",
    "                        })\n",
    "                    \n",
    "                    \n",
    "                elif is_valid_journal(paper):\n",
    "                    paper[\"publicationType\"]=\"JournalArticle\"\n",
    "                    paper[\"venue\"]=None\n",
    "                    paper[\"journalName\"]=paper[\"journal\"][\"name\"]\n",
    "                    paper[\"journalVolume\"]=paper[\"journal\"][\"volume\"]\n",
    "                    if paper[\"journal\"][\"pages\"] is not None:\n",
    "                        paper[\"journalPages\"]=re.sub(r'\\s+', '', paper[\"journal\"][\"pages\"])\n",
    "                    else:\n",
    "                        paper[\"journalPages\"]=None\n",
    "                    if paper[\"publicationId\"]+str(paper[\"year\"]) not in set_joutnals:\n",
    "                        set_joutnals.add(paper[\"publicationId\"]+str(paper[\"year\"]))\n",
    "                        writers['journal'].writerow({\n",
    "                        \"journalId\": paper[\"publicationId\"],\n",
    "                        \"journalName\": paper[\"journalName\"],\n",
    "                        \"journalVolume\": paper[\"journalVolume\"],\n",
    "                        \"journalPages\": paper[\"journalPages\"], \n",
    "                        \"yearId\": paper[\"year\"]-1950,\n",
    "                        })\n",
    "                    \n",
    "                else:\n",
    "                    continue   \n",
    "                paperId = paper.get(\"paperId\")\n",
    "                paper_authors = paper[\"authors\"]\n",
    "                writers['paper'].writerow({\n",
    "                    \"paperId\": paperId,\n",
    "                    \"corpusId\": paper.get(\"corpusId\"),\n",
    "                    \"title\":  paper.get(\"title\").strip().replace(\"\\n\", \" \").replace(\"|\", \" \").replace('\"', \"\").replace(\"^\", \" \"),\n",
    "                    \"referenceAuthorId\": get_referencing_author_id(paper_authors),\n",
    "                    \"abstract\": paper.get(\"abstract\").strip().replace(\"\\n\", \" \").replace(\"|\", \" \").replace('\"', \"\").replace(\"^\", \" \"),\n",
    "                    \"url\": paper.get(\"url\"),\n",
    "                    \"yearId\": paper.get(\"year\")-1950,\n",
    "                    \"publicationType\": paper.get(\"publicationType\"),\n",
    "                    \"publicationDate\": paper.get(\"publicationDate\"),\n",
    "                    \"publicationId\": paper.get(\"publicationId\")\n",
    "                })\n",
    "\n",
    "                new_papers = set([paper['paperId'] for paper in paper['references']])\n",
    "                process_new_papers(processed_papers, to_be_processed_papers, processing_papers_id, new_papers)\n",
    "\n",
    "                for new_paper in new_papers:\n",
    "                    writers['paper_paper'].writerow({\n",
    "                        \"citingPaperId\": paperId,\n",
    "                        \"citedPaperId\": new_paper,\n",
    "                    })\n",
    "\n",
    "                for author in paper_authors:\n",
    "                    authorId = author.get(\"authorId\")\n",
    "                    authorName = author.get(\"name\")\n",
    "                    if authorId and authorName:\n",
    "                        writers['paper_author'].writerow({\n",
    "                            \"paperId\": paperId,\n",
    "                            \"authorId\": authorId\n",
    "                        })\n",
    "\n",
    "                    if authorId and authorName and authorId not in set_authors:\n",
    "                        writers['author'].writerow({\n",
    "                                \"authorId\": authorId,\n",
    "                                \"authorName\": authorName\n",
    "                        })\n",
    "\n",
    "                        set_authors.add(authorId)\n",
    "            \n",
    "                paper_keywords = paper.get(\"s2FieldsOfStudy\", [])\n",
    "                paper_keywords = set(map(lambda x: x['category'], paper_keywords))\n",
    "\n",
    "                for keyword in paper_keywords:\n",
    "                    writers[\"paper_keywords\"].writerow({\n",
    "                            \"paperId\": paperId,\n",
    "                            \"keyword\": keyword\n",
    "                        })\n",
    "                    \n",
    "                    if keyword not in set_keywords:\n",
    "                        writers[\"keywords\"].writerow({\n",
    "                            \"keyword\": keyword\n",
    "                        })\n",
    "\n",
    "                        set_keywords.add(keyword)\n",
    "                \n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_file = \"csv/paper.csv\"\n",
    "conferences_file = \"csv/conference.csv\"\n",
    "journals_file = \"csv/journal.csv\"\n",
    "\n",
    "df=pd.read_csv(papers_file, delimiter='|')\n",
    "df_conferences=pd.read_csv(conferences_file, delimiter='|')\n",
    "df_journals=pd.read_csv(journals_file, delimiter='|')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "conferences=list(set(df_conferences[\"conferenceId\"].to_list()))\n",
    "\n",
    "split_index = len(conferences) // 2\n",
    "\n",
    "work_shops = conferences[split_index:]\n",
    "conferences = conferences[:split_index]\n",
    "\n",
    "df_conf = df_conferences[df_conferences['conferenceId'].isin(conferences)]\n",
    "df_workshops= df_conferences[df_conferences['conferenceId'].isin(work_shops)]\n",
    "df_workshops.rename(columns={'conferenceId': 'workshopId','conferenceName':'workshopName'})\n",
    "df.loc[df['publicationId'].isin(work_shops), 'publicationType'] = 'WorkShop'\n",
    "\n",
    "# Save each filtered DataFrame to a separate CSV file\n",
    "df_conf.to_csv(\"csv/conference.csv\", sep=\"|\",index=False)\n",
    "df_workshops.to_csv(\"csv/workshop.csv\", sep=\"|\", index=False)\n",
    "df.to_csv(\"csv/paper.csv\", sep=\"|\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning references :)\n",
    "citations_file=\"csv/paper_paper.csv\"\n",
    "\n",
    "df_citations=pd.read_csv(citations_file, delimiter='|')\n",
    "df_citations_new = df_citations[df_citations[\"citedPaperId\"].isin(df['paperId'].to_list())]\n",
    "df_citations_new.to_csv(\"csv/paper_paper.csv\", sep='|', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://api.semanticscholar.org/graph/v1/author/batch\"\n",
    "BATCH_SIZE = 500\n",
    "author_file=\"csv/author.csv\"  \n",
    "query_params = {\n",
    "    \"fields\": \"name,url,paperCount,hIndex\"\n",
    "}\n",
    "\n",
    "df=pd.read_csv(author_file,  delimiter='|')\n",
    "ids=df[\"authorId\"].values.tolist()\n",
    "df_copy = df.copy()\n",
    "\n",
    "\n",
    "with open(author_file, \"w\", newline='', encoding=\"utf-8\") as outfile  :   \n",
    "    csv_writer_1 = csv.DictWriter(outfile, fieldnames=[\"authorId\", \"url\", \"authorName\", \"paperCount\", \"hIndex\"], delimiter=\"|\")\n",
    "    # Write the headers to the CSV files\n",
    "    csv_writer_1.writeheader()\n",
    "    for i in range(0, len(ids), BATCH_SIZE):\n",
    "        batch = ids[i:i + BATCH_SIZE]\n",
    "        data = {\n",
    "        \"ids\": batch\n",
    "        }\n",
    "        response = requests.post(url, params=query_params, json=data, headers=headers).json()\n",
    "        # Save the results to json file\n",
    "        count=0\n",
    "        for paper in response:\n",
    "            count+=1\n",
    "            try: \n",
    "                if paper is not None: \n",
    "                    paper_row = {\n",
    "                                \"authorId\": paper.get(\"authorId\"),\n",
    "                                \"url\": paper.get(\"url\"),\n",
    "                                \"authorName\": paper.get(\"name\"),\n",
    "                                \"paperCount\": paper.get(\"paperCount\"),\n",
    "                                \"hIndex\": paper.get(\"hIndex\")\n",
    "                                }\n",
    "                    # Write the row to CSV 1\n",
    "                    csv_writer_1.writerow(paper_row)\n",
    "                else:\n",
    "                    paper_row = {\n",
    "                                \"authorId\":df_copy.loc[df_copy['authorId'] == batch[count-1], \"authorId\"],\n",
    "                                \"url\": None,\n",
    "                                \"authorName\": df_copy.loc[df_copy['authorId'] == batch[count-1], \"authorName\"],\n",
    "                                \"paperCount\": None,\n",
    "                                \"hIndex\": None\n",
    "                                }\n",
    "                    # Write the row to CSV 1\n",
    "                    csv_writer_1.writerow(paper_row)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
