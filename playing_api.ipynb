{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests, json, os, csv, re, urllib.parse\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "from contextlib import ExitStack\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.environ.get('API_KEY')\n",
    "headers = {\"x-api-key\": api_key}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_paper(paper):\n",
    "    return paper['authors'] is not [] and paper['abstract'] is not None and paper[\"s2FieldOfStudy\"] is not None\n",
    "\n",
    "\n",
    "def is_valid_conference(paper):\n",
    "    return paper[\"venue\"] is not None\n",
    "\n",
    "\n",
    "def is_valid_journal(paper):\n",
    "    return paper[\"journal\"] is not None and \"name\" in paper[\"journal\"] and \"pages\" in paper[\"journal\"] and \"volume\" in paper[\"journal\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV Needed\n",
    "- paper (w abstract and relevant author) \n",
    "- paper-paper (n-n)\n",
    "- author \n",
    "- paper-author (n-n)\n",
    "- paper-reviewers (n-n)\n",
    "- keywords\n",
    "- paper-keywords (n-n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = {\n",
    "    \"paper\": [\"sid\",\"paperId\",\"corpusId\", \"title\", \"referenceAuthorId\", \"url\", \"year\", \"referenceCount\", \"citationCount\", \"influentialCitationCount\",\"publicationType\", \"publicationDate\"],\n",
    "    \"paper_paper\": [\"sidPaper\", \"sidPaper\"],\n",
    "    \"author\": [\"sid\", \"authorId\", \"name\"],\n",
    "    \"paper_author\": [\"sidPaper\", \"sidAuthor\"],\n",
    "    \"paper_reviewer\": [\"sidPaper\", \"sidReviewAuthor\"],\n",
    "    \"keywords\": [\"sid\", \"keyword\"],\n",
    "    \"paper_keywords\": [\"sidPaper\", \"sidKeyword\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_folder = Path('csv')\n",
    "with ExitStack() as stack:  # Ensures all files are closed properly\n",
    "    files = {name: stack.enter_context(open(csv_folder / (name + '.csv'), \"w\", newline='', encoding=\"utf-8\")) for name in csv_files}\n",
    "    writers = {name: csv.DictWriter(files[name], fieldnames=fieldnames) for name, fieldnames in csv_files.items()}\n",
    "\n",
    "    # Write headers for each CSV file\n",
    "    for writer in writers.values():\n",
    "        writer.writeheader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#********************************************************************************************************************\n",
    "RECORDS = 50  # Number of records to save per category \n",
    "QUERY = \"data\"  # Query to filter the papers\n",
    "FIELDS = \"paperId,corpusId,title,abstract,authors,url,year,referenceCount,citationCount,influentialCitationCount,s2FieldsOfStudy,publicationDate,journal,venue,publicationVenue\"  # Fields to retrieve from the API\n",
    "#********************************************************************************************************************\n",
    "\n",
    "query_encoded = urllib.parse.quote(QUERY)\n",
    "fields_encoded = urllib.parse.quote(FIELDS)\n",
    "\n",
    "type_encoded = urllib.parse.quote(\"JournalArticle\")\n",
    "url1=\"https://api.semanticscholar.org/graph/v1/paper/search?query=\"+query_encoded+\"&publicationTypes=\"+type_encoded+\"&fields=\"+fields_encoded+\"&limit=\"+str(RECORDS)\n",
    "response = requests.get(url1, headers=headers).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'authorId': '2113243762', 'name': 'Hugo Touvron'},\n",
       " {'authorId': '51021910', 'name': 'M. Cord'},\n",
       " {'authorId': '3271933', 'name': 'Matthijs Douze'},\n",
       " {'authorId': '1403239967', 'name': 'Francisco Massa'},\n",
       " {'authorId': '3469062', 'name': 'Alexandre Sablayrolles'},\n",
       " {'authorId': '2065248680', 'name': \"Herv'e J'egou\"}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = response['data']\n",
    "data[0]['authors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m type_encoded \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mparse\u001b[38;5;241m.\u001b[39mquote(publication_type)\n\u001b[1;32m     27\u001b[0m url1\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://api.semanticscholar.org/graph/v1/paper/search?query=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mquery_encoded\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m&publicationTypes=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mtype_encoded\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m&fields=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mfields_encoded\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m&limit=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(RECORDS)\n\u001b[0;32m---> 28\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mjson()\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m     30\u001b[0m     count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/python_venv/general/lib/python3.13/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/python_venv/general/lib/python3.13/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/python_venv/general/lib/python3.13/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/python_venv/general/lib/python3.13/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/python_venv/general/lib/python3.13/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/python_venv/general/lib/python3.13/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/python_venv/general/lib/python3.13/site-packages/urllib3/connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m~/python_venv/general/lib/python3.13/site-packages/urllib3/connection.py:516\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    513\u001b[0m _shutdown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    515\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    519\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py:1430\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1428\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1429\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1430\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1431\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1432\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py:292\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 292\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/socket.py:719\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot read from timed out object\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 719\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py:1304\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1300\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1301\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1302\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1303\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1304\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py:1138\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1138\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1139\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1140\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "output_file = \"csv/papers.csv\"  \n",
    "keywords_file=\"csv/keywords.csv\"\n",
    "\n",
    "api_key = os.environ.get('API_KEY')\n",
    "headers = {\"x-api-key\": api_key}\n",
    "\n",
    "#********************************************************************************************************************\n",
    "RECORDS = 50  # Number of records to save per category \n",
    "PUBLICATION_TYPES = [\"Conference\", \"JournalArticle\"]  # Publication types to filter (There is not Workshop)\n",
    "QUERY = \"data\"  # Query to filter the papers\n",
    "FIELDS = \"paperId,corpusId,title,abstract,authors,url,year,referenceCount,citationCount,influentialCitationCount,s2FieldsOfStudy,publicationDate,journal,venue,publicationVenue\"  # Fields to retrieve from the API\n",
    "#********************************************************************************************************************\n",
    "\n",
    "query_encoded = urllib.parse.quote(QUERY)\n",
    "fields_encoded = urllib.parse.quote(FIELDS)\n",
    "\n",
    "count = 0\n",
    "\n",
    "with open(output_file, \"w\", newline='', encoding=\"utf-8\") as outfile, open(keywords_file, \"w\", newline='', encoding=\"utf-8\") as keyfile  :   \n",
    "    csv_writer_1 = csv.DictWriter(outfile, fieldnames=[\"sid\",\"paperId\",\"corpusId\", \"title\", \"authorId\", \"authorName\", \"url\", \"year\", \"referenceCount\", \"citationCount\", \"influentialCitationCount\",\"publicationType\", \"publicationDate\"])\n",
    "    csv_writer_2 = csv.DictWriter(keyfile, fieldnames=[\"sid\",\"keyword\"])\n",
    "    # Write the headers to the CSV files\n",
    "    csv_writer_1.writeheader()\n",
    "    csv_writer_2.writeheader()\n",
    "    for publication_type in PUBLICATION_TYPES:\n",
    "        type_encoded = urllib.parse.quote(publication_type)\n",
    "        url1=\"https://api.semanticscholar.org/graph/v1/paper/search?query=\"+query_encoded+\"&publicationTypes=\"+type_encoded+\"&fields=\"+fields_encoded+\"&limit=\"+str(RECORDS)\n",
    "        response = requests.get(url1, headers=headers).json()\n",
    "        for line in response[\"data\"]:\n",
    "            count += 1\n",
    "            try:  \n",
    "                if not is_valid_paper():\n",
    "                    continue\n",
    "                if publication_type==\"Conference\":\n",
    "                    if is_valid_conference(line):\n",
    "                        line[\"publicationType\"]=\"Conference\"\n",
    "                        line[\"journalName\"]=None\n",
    "                        line[\"journalVolume\"]=None\n",
    "                        line[\"journalPages\"]=None\n",
    "                    else:\n",
    "                        continue\n",
    "                elif publication_type==\"JournalArticle\":\n",
    "                    if is_valid_journal(line):\n",
    "                        line[\"publicationType\"]=\"JournalArticle\"\n",
    "                        line[\"venue\"]=None\n",
    "                        line[\"journalName\"]=line[\"journal\"][\"name\"]\n",
    "                        line[\"journalVolume\"]=line[\"journal\"][\"volume\"]\n",
    "                        if line[\"journal\"][\"pages\"] is not None:\n",
    "                            line[\"journalPages\"]=re.sub(r'\\s+', '', line[\"journal\"][\"pages\"])\n",
    "                        else:\n",
    "                            line[\"journalPages\"]=None\n",
    "                    else:\n",
    "                        continue    \n",
    "                line[\"authorId\"] = line[\"authors\"][0][\"authorId\"]\n",
    "                line[\"authorName\"] = line[\"authors\"][0][\"name\"]\n",
    "                row_papers = {\n",
    "                    \"sid\": count, # Add a new column with a surrogated ID, just in case\n",
    "                    \"paperId\": line.get(\"paperId\"),\n",
    "                    \"corpusId\": line.get(\"corpusId\"),\n",
    "                    \"title\":  line.get(\"title\").strip().replace(\"\\n\", \" \"),\n",
    "                    \"authorId\": line.get(\"authorId\"),\n",
    "                    \"authorName\": line.get(\"authorName\"),\n",
    "                    \"url\": line.get(\"url\"),\n",
    "                    \"year\": line.get(\"year\"),\n",
    "                    \"referenceCount\": line.get(\"referenceCount\"),\n",
    "                    \"citationCount\": line.get(\"citationCount\"),\n",
    "                    \"influentialCitationCount\": line.get(\"influentialCitationCount\"),\n",
    "                    \"publicationType\": line.get(\"publicationType\"),\n",
    "                    \"publicationDate\": line.get(\"publicationDate\"),\n",
    "                }\n",
    "                csv_writer_1.writerow(row_papers)\n",
    "                \n",
    "                keywords=list()\n",
    "                for keyword in line.get(\"s2FieldsOfStudy\", []):\n",
    "                    keywords.append(keyword[\"category\"])\n",
    "                keywords = list(set(keywords))\n",
    "                for keyword in keywords:\n",
    "                    row_keywords = {\n",
    "                        \"sid\": count, # We will use the surrogated ID here, just in case\n",
    "                        \"keyword\": keyword\n",
    "                    }\n",
    "                    # Write the row to CSV 2 for each keyword\n",
    "                    csv_writer_2.writerow(row_keywords)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON: {e}\")\n",
    "print(f\"Modified JSONL saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"paperId\": \"ad7ddcc14984caae308c397f1a589aae75d4ab71\",\n",
      "    \"references\": [\n",
      "      {\n",
      "        \"paperId\": \"cec7872b194aadf54140578b9be52939eb1112e9\",\n",
      "        \"title\": \"LambdaNetworks: Modeling Long-Range Interactions Without Attention\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"b52431a4268bd2f848db4a0c8c614dc1e687eeab\",\n",
      "        \"title\": \"Grafit: Learning fine-grained image representations with coarse labels\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"268d347e8a55b5eb82fb5e7d2f800e33c75ab18a\",\n",
      "        \"title\": \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"64b9be00f4eecd465b4e8e46e2ab7624d7eaeb2b\",\n",
      "        \"title\": \"Global Self-Attention Networks for Image Recognition\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"867ec3a4837213d0096fec75aa6d1dbbfd2c4b1d\",\n",
      "        \"title\": \"Feature Space Augmentation for Long-Tailed Data\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"bc022dbb37b1bbf3905a7404d19c03ccbf6b81a8\",\n",
      "        \"title\": \"Generative Pretraining From Pixels\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"5156381d63bb3e873533b08f203cb56c2d79b6c9\",\n",
      "        \"title\": \"Object-Centric Learning with Slot Attention\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"f5c8464032a936451b222be1984cabf42d6adfa8\",\n",
      "        \"title\": \"Are we done with ImageNet?\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"a0185d4f32dde88aa1749f3a8000ed4721787b65\",\n",
      "        \"title\": \"Visual Transformers: Token-based Image Representation and Processing for Computer Vision\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"13da774fe604027bff2951ba82f4c3d9be7e415e\",\n",
      "        \"title\": \"Augment Your Batch: Improving Generalization Through Instance Repetition\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"5335fe1bf347f7ad1dce1611ea4b60bd8391a090\",\n",
      "        \"title\": \"Transferring Inductive Biases through Knowledge Distillation\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"962dc29fdc3fbdc5930a10aba114050b82fe5a3e\",\n",
      "        \"title\": \"End-to-End Object Detection with Transformers\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"fb93ca1e004cbdcb93c8ffc57357189fa4eb6770\",\n",
      "        \"title\": \"ResNeSt: Split-Attention Networks\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"0171ad4cc87cc7db25b4ec3169e293eed9a13b39\",\n",
      "        \"title\": \"Training with Quantization Noise for Extreme Model Compression\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"2709167f1c3a03fa5b970a665ea48ed243aab582\",\n",
      "        \"title\": \"Designing Network Design Spaces\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"b3e5bac38d098378ba24f826595b8ea877e342f6\",\n",
      "        \"title\": \"Circumventing Outliers of AutoAugment with Knowledge Distillation\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"a4ede8d8aa9482f316669c1fecd56c41e8da01de\",\n",
      "        \"title\": \"Fixing the train-test resolution discrepancy: FixEfficientNet\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"3c8a456509e6c0805354bd40a35e3f2dbf8069b1\",\n",
      "        \"title\": \"PyTorch: An Imperative Style, High-Performance Deep Learning Library\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"20ba55ee3229db5cb190a00e788c59f08d2a767d\",\n",
      "        \"title\": \"Self-Training With Noisy Student Improves ImageNet Classification\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"bb713d56a39a040b35e4f9e036fb4422f543e614\",\n",
      "        \"title\": \"On the Relationship between Self-Attention and Convolutional Layers\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"8de7f044a673d1f5e3b454d0663811f91aa9811a\",\n",
      "        \"title\": \"On the Efficacy of Knowledge Distillation\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"87f6a7c014ce206ac5b57299c07e10667d194b39\",\n",
      "        \"title\": \"Randaugment: Practical automated data augmentation with a reduced search space\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"dfc7b58b67c31932b48586b3e23a43cc94695290\",\n",
      "        \"title\": \"UNITER: UNiversal Image-TExt Representation Learning\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"f4a8480cffa491020bdbb8c4c4e7a7e923b1c2c1\",\n",
      "        \"title\": \"Reducing Transformer Depth on Demand with Structured Dropout\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"bf96f5c2d68f73d3f4d45603a5ccb803cdd92ea8\",\n",
      "        \"title\": \"Revisit Knowledge Distillation: a Teacher-free Framework\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"5aec474c31a2f4b74703c6f786c0a8ff85c450da\",\n",
      "        \"title\": \"VisualBERT: A Simple and Performant Baseline for Vision and Language\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"65a9c7b0800c86a196bc14e7621ff895cc6ab287\",\n",
      "        \"title\": \"ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"c0aaee2337e5af680e5dca1bfc349a737dfec573\",\n",
      "        \"title\": \"Fixing the train-test resolution discrepancy\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"d6dccb5d71fbb6f5765f89633ba3a8e6809a720d\",\n",
      "        \"title\": \"Stand-Alone Self-Attention in Vision Models\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9\",\n",
      "        \"title\": \"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"ed17929e66da7f8fbc3666bf5eb613d302ddde0c\",\n",
      "        \"title\": \"CutMix: Regularization Strategy to Train Strong Classifiers With Localizable Features\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"27ac832ee83d8b5386917998a171a0257e2151e2\",\n",
      "        \"title\": \"Attention Augmented Convolutional Networks\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"c41a11c0e9b8b92b4faaf97749841170b760760a\",\n",
      "        \"title\": \"VideoBERT: A Joint Model for Video and Language Representation Learning\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"fb8cf663a71bf31f59557a35d36aaf8c465b50af\",\n",
      "        \"title\": \"Selective Kernel Networks\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"1e7678467b1807777dcd9be557b79328ce9419a8\",\n",
      "        \"title\": \"MultiGrain: a unified image embedding for classes and instances\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"4e0bb8c1c683b43357c5d5216f6b74ff2cb32434\",\n",
      "        \"title\": \"Do ImageNet Classifiers Generalize to ImageNet?\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"29309743870c825f9645a4803af727402462e513\",\n",
      "        \"title\": \"Bag of Tricks for Image Classification with Convolutional Neural Networks\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"f723eb3e7159f07b97464c8d947d15e78612abe4\",\n",
      "        \"title\": \"AutoAugment: Learning Augmentation Policies from Data\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"d23a1cd6c73e3e43295c3585b3db147eb1c3ee91\",\n",
      "        \"title\": \"How to Start Training: The Effect of Initialization and Architecture\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"6a0aaefce8a27a8727d896fa444ba27558b2d381\",\n",
      "        \"title\": \"Relation Networks for Object Detection\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"8899094797e82c5c185a0893896320ef77f60e64\",\n",
      "        \"title\": \"Non-local Neural Networks\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"45dfef0cc1ed96558c1c650432ce39d6a1050b6a\",\n",
      "        \"title\": \"Fixing Weight Decay Regularization in Adam\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"4feef0fd284feb1233399b400eb897f59ec92755\",\n",
      "        \"title\": \"mixup: Beyond Empirical Risk Minimization\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"fb37561499573109fc2cebb6a7b08f44917267dd\",\n",
      "        \"title\": \"Squeeze-and-Excitation Networks\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"2788a2461ed0067e2f7aaa63c449a24a237ec341\",\n",
      "        \"title\": \"Random Erasing Data Augmentation\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"8760bc7631c0cb04e7138254e9fd6451b7def8ca\",\n",
      "        \"title\": \"Revisiting Unreasonable Effectiveness of Data in Deep Learning Era\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"204e3073870fae3d05bcbc2f6a8e263d9b72e776\",\n",
      "        \"title\": \"Attention is All you Need\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"0d57ba12a6d958e178d83be4c84513f7e42b24e5\",\n",
      "        \"title\": \"Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"43428880d75b3a14257c3ee9bda054e61eb869c0\",\n",
      "        \"title\": \"Convolutional Sequence to Sequence Learning\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"97fb4e3d45bb098e27e0071448b6152217bd35a5\",\n",
      "        \"title\": \"Layer Normalization\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"de5e7320729f5d3cbb6709eb6329ec41ace8c95d\",\n",
      "        \"title\": \"Gaussian Error Linear Units (GELUs)\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"51db1f3c8dfc7d4077da39c96bb90a6358128111\",\n",
      "        \"title\": \"Deep Networks with Stochastic Depth\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"2c03df8b48bf3fa39054345bafabfeff15bfd11d\",\n",
      "        \"title\": \"Deep Residual Learning for Image Recognition\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"23ffaa0fe06eae05817f527a47ac3291077f9e58\",\n",
      "        \"title\": \"Rethinking the Inception Architecture for Computer Vision\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"0c908739fbff75f03469d13d4a1a07de3414ee19\",\n",
      "        \"title\": \"Distilling the Knowledge in a Neural Network\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"eb42cf88027de515750f230b23b1a057dc782108\",\n",
      "        \"title\": \"Very Deep Convolutional Networks for Large-Scale Image Recognition\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"e74f9b7f8eec6ba4704c206b93bc8079af3da4bd\",\n",
      "        \"title\": \"ImageNet Large Scale Visual Recognition Challenge\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"a83cec6a91701bd8500f8c43ad731d4353c71d55\",\n",
      "        \"title\": \"3D Object Representations for Fine-Grained Categorization\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"abd1c342495432171beb7ca8fd9551ef13cbd0ff\",\n",
      "        \"title\": \"ImageNet classification with deep convolutional neural networks\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"d2c733e34d48784a37d717fe43d9e93277a8c53e\",\n",
      "        \"title\": \"ImageNet: A large-scale hierarchical image database\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"02b28f3b71138a06e40dbd614abf8568420ae183\",\n",
      "        \"title\": \"Automated Flower Classification over a Large Number of Classes\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": null,\n",
      "        \"title\": \"Pytorch image models\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"df2b0e26d0599ce3e70df8a9da02e51594e0e992\",\n",
      "        \"title\": \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": null,\n",
      "        \"title\": \"Edouard\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": null,\n",
      "        \"title\": \"The inaturalist challenge\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"5d90f06bb70a0a3dced62413346235c02b1aa086\",\n",
      "        \"title\": \"Learning Multiple Layers of Features from Tiny Images\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"paperId\": \"2582ab7c70c9e7fcb84545944eba8f3a7f253248\",\n",
      "    \"references\": [\n",
      "      {\n",
      "        \"paperId\": \"87f40e6f3022adbc1f1905e3e506abad05a9964f\",\n",
      "        \"title\": \"Distributed Representations of Words and Phrases and their Compositionality\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"834cb8e1e738b8d2c6d24e652ac966d6e7089a46\",\n",
      "        \"title\": \"Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"8007fc25a1f5c03f7c8ac95ccf5cf8aa3d989092\",\n",
      "        \"title\": \"Learning New Facts From Knowledge Bases With Neural Tensor Networks and Semantic Word Vectors\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"eb6208f3e2c0942e38ceffc443dcf64d2cb4ec82\",\n",
      "        \"title\": \"A semantic matching energy function for learning with multi-relational data\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"04cc04457e09e17897f9256c86b45b92d70a401f\",\n",
      "        \"title\": \"A latent factor model for highly multi-relational data\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"5c12193fc84ed7973fe4515ae893625d8af4ce4f\",\n",
      "        \"title\": \"Max-Margin Nonparametric Latent Feature Models for Link Prediction\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"498ca0a1f8c980586408addf7ab2919ecdb7dd3d\",\n",
      "        \"title\": \"Factorizing YAGO: scalable machine learning for linked data\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"1f4a4769e4d2fb846e59c2f185e0377190739f18\",\n",
      "        \"title\": \"Learning Structured Embeddings of Knowledge Bases\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"f6764d853a14b0c34df1d2283e76277aead40fde\",\n",
      "        \"title\": \"A Three-Way Model for Collective Learning on Multi-Relational Data\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"ea9d2a2b4ce11aaf85136840c65f3bc9c03ab649\",\n",
      "        \"title\": \"Understanding the difficulty of training deep feedforward neural networks\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"81bbe42e3ec09c28b8864956148e58f4cb5aa860\",\n",
      "        \"title\": \"Modelling Relational Data using Bayesian Clustered Tensor Factorization\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"4e07791ee0872401215f12aefde342bd843240cc\",\n",
      "        \"title\": \"Nonparametric Latent Feature Models for Link Prediction\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"fec691d09b564986ad27162ce15344604c840ff9\",\n",
      "        \"title\": \"Relational learning via collective matrix factorization\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"1976c9eeccc7115d18a04f1e7fb5145db6b96002\",\n",
      "        \"title\": \"Freebase: a collaboratively created graph database for structuring human knowledge\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"87ca5a0f345533c30217f6359bc4325a2442a0b9\",\n",
      "        \"title\": \"Learning Systems of Concepts with an Infinite Relational Model\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"68c03788224000794d5491ab459be0b2a2c38677\",\n",
      "        \"title\": \"WordNet: A Lexical Database for English\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"9157970408f931a45a7d245ac909ad7f1c2558bd\",\n",
      "        \"title\": \"PARAFAC: parallel factor analysis\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": null,\n",
      "        \"title\": \"HEAD AND LABEL) PREDICTED TAILS\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"paperId\": \"f4ba954b0412773d047dc41231c733de0c1f4926\",\n",
      "    \"references\": [\n",
      "      {\n",
      "        \"paperId\": \"3fab92869cfab684b3ffb1c16a771e9c3b774acd\",\n",
      "        \"title\": \"The Use of Classifiers in Sequential Inference\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"02c8a0bc8bab9920e6615cfacf1df2ab3f2b1f68\",\n",
      "        \"title\": \"Information Extraction with HMM Structures Learned by Stochastic Optimization\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"bece46ed303f8eaef2affae2cba4e0aef51fe636\",\n",
      "        \"title\": \"Maximum Entropy Markov Models for Information Extraction and Segmentation\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"844db702be4bc149b06b822b47247e15f5894cc3\",\n",
      "        \"title\": \"Discriminative Reranking for Natural Language Parsing\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"f71c68f0c7a94851f06de396dff2e2588ad44768\",\n",
      "        \"title\": \"Proceedings of the Thirteenth Annual Conference on Computational Learning Theory\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"1720d537c7378d0e9e37896fd01bd361a1b7f623\",\n",
      "        \"title\": \"Minimization algorithms for sequential transducers\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"084c55d6432265785e3ff86a2e900a49d501c00a\",\n",
      "        \"title\": \"Book Reviews: Foundations of Statistical Natural Language Processing\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"3ed17a1114e2dc48597ab17cc8d5234006f525c9\",\n",
      "        \"title\": \"Learning to Resolve Natural Language Ambiguities: A Unified Approach\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"d30cf4154da4dc8c8073c2f59cc35b1e7d453a65\",\n",
      "        \"title\": \"Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"f72084efcae8b2007e590b0c5a8f1decb61ef935\",\n",
      "        \"title\": \"A whole sentence maximum entropy language model\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"4ba566223e426677d12a9a18418c023a4deec77e\",\n",
      "        \"title\": \"A decision-theoretic generalization of on-line learning and an application to boosting\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"f4cc5563c694355ddcf746ff9a55ccdb22d86a98\",\n",
      "        \"title\": \"Finite-State Transducers in Language and Speech Processing\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"fb486e03369a64de2d5b0df86ec0a7b55d3907db\",\n",
      "        \"title\": \"A Maximum Entropy Approach to Natural Language Processing\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"2ffaf95de19deeacda2a9ea385adc0e4a3f95d3f\",\n",
      "        \"title\": \"Transformation-Based Error-Driven Learning and Natural Language Processing: A Case Study in Part-of-Speech Tagging\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"b951b9f78b98a186ba259027996a48e4189d37e5\",\n",
      "        \"title\": \"Inducing Features of Random Fields\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"307142db035938bef49b232e371238d895db4df2\",\n",
      "        \"title\": \"A comparison of several approximate algorithms for finding multiple (N-best) sentence hypotheses\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"37c931cbaa9217b829596dd196520a838562a109\",\n",
      "        \"title\": \"Generalized Iterative Scaling for Log-Linear Models\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"cb0690094be9d21334745f917b0adc4d87e0e898\",\n",
      "        \"title\": \"Efficient Training of Conditional Random Fields\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"9956f29b45ec60463511cb24bafdb2380e6b1aad\",\n",
      "        \"title\": \"An Introduction to Probabilistic Automata\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"f42b865e20e61a954239f421b42007236e671f19\",\n",
      "        \"title\": \"GradientBased Learning Applied to Document Recognition\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": null,\n",
      "        \"title\": \"2000).Logistic regression,AdaBoost,andBregmandistances.Proc.13th COLT\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"5db4c0f527adc11923c97a05e947e21711572a97\",\n",
      "        \"title\": \"Boosting Applied to Tagging and PP Attachment\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"162d958ff885f1462aeda91cd72582323fd6a1f4\",\n",
      "        \"title\": \"Gradient-based learning applied to document recognition\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"a574e320d899e7e82e341eb64baef7dfe8a24642\",\n",
      "        \"title\": \"A Maximum Entropy Model for Part-Of-Speech Tagging\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"b579e6516d9e27949b74d37e40f24c136aba9b0b\",\n",
      "        \"title\": \"Equivalence of Linear Boltzmann Chains and Hidden Markov Models\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"2dc4d6d7d55f9f0f1de53bb7f6816502f8f38892\",\n",
      "        \"title\": \"Boltzmann Chains and Hidden Markov Models\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": null,\n",
      "        \"title\": \"Une approche th\\u00b4eorique de l\\u2019apprentissage connexionniste: Applications `a la reconnaissance de la parole\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"ec75e3ca906681bd900218a348a4a35dfed3d6fd\",\n",
      "        \"title\": \"Markov fields on finite graphs and lattices\"\n",
      "      },\n",
      "      {\n",
      "        \"paperId\": \"a830f32fba171482ed9d187168fbf5b691524ab5\",\n",
      "        \"title\": \"Introduction to Probabilistic Automata\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfor paper_id, sid in zip(df[\"paperId\"], df[\"sid\"]): \\n    url = \"https://api.semanticscholar.org/graph/v1/paper/\"+str(paper_id)+\"/references?limit=10\"\\n    response = requests.get(url, headers=headers).json()\\n    for line in response[\"data\"]:\\n        dic_cite.append({\"sid\":sid,\"citingPaperId\":line[\"citedPaper\"][\"paperId\"]})\\n        \\nprint(dic_cite)\\n\\n\\nfor id in ids: \\n    url = \"https://api.semanticscholar.org/graph/v1/\"+str(id)+\"/citations\"\\n    print(url)\\n\\n    response = requests.get(url, headers=headers).json()\\n    print(json.dumps(response, indent=2))\\n# Save the results to json file\\n\\nwith open(output_file, \"w\", newline=\\'\\', encoding=\"utf-8\") as outfile  :   \\n    csv_writer_1 = csv.DictWriter(outfile, fieldnames=[\"sid\",\"authorId\", \"url\", \"name\", \"paperCount\", \"hIndex\"])\\n\\n    # Write the headers to the CSV files\\n    csv_writer_1.writeheader()\\n    for line in response:\\n        count+=1\\n        try:  \\n            row_papers = {\\n                        \"sid\": count, # Add a new column with a surrogated ID, just in case\\n                        \"authorId\": line.get(\"authorId\"),\\n                        \"url\": line.get(\"url\"),\\n                        \"name\": line.get(\"name\"),\\n                        \"paperCount\": line.get(\"paperCount\"),\\n                        \"hIndex\": line.get(\"hIndex\")\\n                        }\\n            # Write the row to CSV 1\\n            csv_writer_1.writerow(row_papers)\\n        except json.JSONDecodeError as e:\\n            print(f\"Error decoding JSON: {e}\")\\nprint(f\"Modified JSONL saved to {output_file}\")\\n'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_file=\"csv/citations.csv\"\n",
    "papers_file = \"csv/papers.csv\"  \n",
    "\n",
    "df=pd.read_csv(papers_file)\n",
    "dic_cite=[]\n",
    "\n",
    "api_key = os.environ.get('API_KEY')\n",
    "headers = {\"x-api-key\": api_key}\n",
    "\n",
    "ids=df[\"paperId\"][0:3].tolist()\n",
    "\n",
    "\n",
    "r = requests.post(\n",
    "    'https://api.semanticscholar.org/graph/v1/paper/batch',\n",
    "    params={'fields': 'references'},\n",
    "    json={\"ids\": ids}\n",
    ")\n",
    "\n",
    "print(json.dumps(r.json(), indent=2))\n",
    "\n",
    "\"\"\"\n",
    "for paper_id, sid in zip(df[\"paperId\"], df[\"sid\"]): \n",
    "    url = \"https://api.semanticscholar.org/graph/v1/paper/\"+str(paper_id)+\"/references?limit=10\"\n",
    "    response = requests.get(url, headers=headers).json()\n",
    "    for line in response[\"data\"]:\n",
    "        dic_cite.append({\"sid\":sid,\"citingPaperId\":line[\"citedPaper\"][\"paperId\"]})\n",
    "        \n",
    "print(dic_cite)\n",
    "\n",
    "\n",
    "for id in ids: \n",
    "    url = \"https://api.semanticscholar.org/graph/v1/\"+str(id)+\"/citations\"\n",
    "    print(url)\n",
    "\n",
    "    response = requests.get(url, headers=headers).json()\n",
    "    print(json.dumps(response, indent=2))\n",
    "# Save the results to json file\n",
    "\n",
    "with open(output_file, \"w\", newline='', encoding=\"utf-8\") as outfile  :   \n",
    "    csv_writer_1 = csv.DictWriter(outfile, fieldnames=[\"sid\",\"authorId\", \"url\", \"name\", \"paperCount\", \"hIndex\"])\n",
    "\n",
    "    # Write the headers to the CSV files\n",
    "    csv_writer_1.writeheader()\n",
    "    for line in response:\n",
    "        count+=1\n",
    "        try:  \n",
    "            row_papers = {\n",
    "                        \"sid\": count, # Add a new column with a surrogated ID, just in case\n",
    "                        \"authorId\": line.get(\"authorId\"),\n",
    "                        \"url\": line.get(\"url\"),\n",
    "                        \"name\": line.get(\"name\"),\n",
    "                        \"paperCount\": line.get(\"paperCount\"),\n",
    "                        \"hIndex\": line.get(\"hIndex\")\n",
    "                        }\n",
    "            # Write the row to CSV 1\n",
    "            csv_writer_1.writerow(row_papers)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON: {e}\")\n",
    "print(f\"Modified JSONL saved to {output_file}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified JSONL saved to csv/authors.csv\n"
     ]
    }
   ],
   "source": [
    "url = \"https://api.semanticscholar.org/graph/v1/author/batch\"\n",
    "output_file=\"csv/authors.csv\"\n",
    "papers_file = \"csv/papers.csv\" \n",
    "count=0  \n",
    "query_params = {\n",
    "    \"fields\": \"name,url,paperCount,hIndex\"#,papers\"\n",
    "}\n",
    "\n",
    "df=pd.read_csv(papers_file)\n",
    "ids=df[\"authorId\"].values.tolist()\n",
    "\n",
    "data = {\n",
    "    \"ids\": ids\n",
    "}\n",
    "api_key = os.environ.get('API_KEY')\n",
    "headers = {\"x-api-key\": api_key}\n",
    "\n",
    "# Send the API request\n",
    "response = requests.post(url, params=query_params, json=data, headers=headers).json()\n",
    "# Save the results to json file\n",
    "with open(output_file, \"w\", newline='', encoding=\"utf-8\") as outfile  :   \n",
    "    csv_writer_1 = csv.DictWriter(outfile, fieldnames=[\"sid\",\"authorId\", \"url\", \"name\", \"paperCount\", \"hIndex\"])\n",
    "\n",
    "    # Write the headers to the CSV files\n",
    "    csv_writer_1.writeheader()\n",
    "    for line in response:\n",
    "        count+=1\n",
    "        try:  \n",
    "            row_papers = {\n",
    "                        \"sid\": count, # Add a new column with a surrogated ID, just in case\n",
    "                        \"authorId\": line.get(\"authorId\"),\n",
    "                        \"url\": line.get(\"url\"),\n",
    "                        \"name\": line.get(\"name\"),\n",
    "                        \"paperCount\": line.get(\"paperCount\"),\n",
    "                        \"hIndex\": line.get(\"hIndex\")\n",
    "                        }\n",
    "            # Write the row to CSV 1\n",
    "            csv_writer_1.writerow(row_papers)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON: {e}\")\n",
    "print(f\"Modified JSONL saved to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
