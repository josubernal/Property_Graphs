{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "import urllib.parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the databases of the API with the following code. We will be using the oldest version of the databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"release_id\": \"2022-05-10\",\n",
      "  \"README\": \"Semantic Scholar Academic Graph Datasets\\n\\nThese datasets provide a variety of information about research papers taken from a snapshot in time of the Semantic Scholar corpus.\\n\\nThis site is provided by The Allen Institute for Artificial Intelligence (\\u201cAI2\\u201d) as a service to the\\nresearch community. The site is covered by AI2 Terms of Use and Privacy Policy. AI2 does not claim\\nownership of any materials on this site unless specifically identified. AI2 does not exercise editorial\\ncontrol over the contents of this site. AI2 respects the intellectual property rights of others. If\\nyou believe your copyright or trademark is being infringed by something on this site, please follow\\nthe \\\"DMCA Notice\\\" process set out in the Terms of Use (https://allenai.org/terms).\\n\\nSAMPLE DATA ACCESS\\nSample data files can be downloaded with the following UNIX command:\\n\\nfor f in $(curl https://s3-us-west-2.amazonaws.com/ai2-s2ag/samples/MANIFEST.txt)\\n  do curl --create-dirs \\\"https://s3-us-west-2.amazonaws.com/ai2-s2ag/$f\\\" -o $f\\ndone\\n\\nFULL DATA ACCESS\\nDownloading the full data requires an API key, which can be obtained at https://www.semanticscholar.org/product/api#Partner-Form\\nFor access to the full datasets, see https://api.semanticscholar.org/api-docs/datasets.\\n\\nLICENSE and ATTRIBUTION\\n\\nSee the README files for each dataset for information about licensing and attribution.\",\n",
      "  \"datasets\": [\n",
      "    {\n",
      "      \"name\": \"abstracts\",\n",
      "      \"description\": \"Paper abstract text, where available.\\n100M records in 30 1.8GB files.\",\n",
      "      \"README\": \"Semantic Scholar Academic Graph Datasets\\n\\nThe \\\"abstracts\\\" dataset provides abstract text for selected papers.\\n\\nSCHEMA\\n - openAccessInfo\\n   - externalIds: IDs of this paper in different catalogs\\n   - license/url/status: open-access information provided by Unpaywall, linked by DOI or PubMed Central ID\\n\\nLICENSE\\nThis collection is licensed under ODC-BY. (https://opendatacommons.org/licenses/by/1.0/)\\n\\nBy downloading this data you acknowledge that you have read and agreed to all the terms in this license.\\n\\nATTRIBUTION\\nWhen using this data in a product or service, or including data in a redistribution, please cite the following paper:\\n\\nBibTex format:\\n@inproceedings{Ammar2018ConstructionOT,\\n  title={Construction of the Literature Graph in Semantic Scholar},\\n  author={Waleed Ammar and Dirk Groeneveld and Chandra Bhagavatula and Iz Beltagy and Miles Crawford and Doug Downey and Jason Dunkelberger and Ahmed Elgohary and Sergey Feldman and Vu A. Ha and Rodney Michael Kinney and Sebastian Kohlmeier and Kyle Lo and Tyler C. Murray and Hsu-Han Ooi and Matthew E. Peters and Joanna L. Power and Sam Skjonsberg and Lucy Lu Wang and Christopher Wilhelm and Zheng Yuan and Madeleine van Zuylen and Oren Etzioni},\\n  booktitle={NAACL},\\n  year={2018}\\n}\\n\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"authors\",\n",
      "      \"description\": \"The core attributes of an author (name, affiliation, paper count, etc.). Authors have an \\\"authorId\\\" field, which can be joined to the \\\"authorId\\\" field of the members of a paper's \\\"authors\\\" field.\\n75M records in 30 100MB files.\",\n",
      "      \"README\": \"Semantic Scholar Academic Graph Datasets\\n\\nThe \\\"authors\\\" dataset provides summary information about authors.\\n\\nSCHEMA\\nSee https://api.semanticscholar.org/api-docs/graph#tag/Author-Data\\n\\nThis dataset does not contain information about an author's papers.\\nInstead, join with authors.authorId from the \\\"papers\\\" dataset.\\n\\nLICENSE\\nThis collection is licensed under ODC-BY. (https://opendatacommons.org/licenses/by/1.0/)\\n\\nBy downloading this data you acknowledge that you have read and agreed to all the terms in this license.\\n\\nATTRIBUTION\\nWhen using this data in a product or service, or including data in a redistribution, please cite the following paper:\\n\\nBibTex format:\\n@inproceedings{Ammar2018ConstructionOT,\\n  title={Construction of the Literature Graph in Semantic Scholar},\\n  author={Waleed Ammar and Dirk Groeneveld and Chandra Bhagavatula and Iz Beltagy and Miles Crawford and Doug Downey and Jason Dunkelberger and Ahmed Elgohary and Sergey Feldman and Vu A. Ha and Rodney Michael Kinney and Sebastian Kohlmeier and Kyle Lo and Tyler C. Murray and Hsu-Han Ooi and Matthew E. Peters and Joanna L. Power and Sam Skjonsberg and Lucy Lu Wang and Christopher Wilhelm and Zheng Yuan and Madeleine van Zuylen and Oren Etzioni},\\n  booktitle={NAACL},\\n  year={2018}\\n}\\n\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"citations\",\n",
      "      \"description\": \"Instances where the bibliography of one paper (the \\\"citingPaper\\\") mentions another paper (the \\\"citedPaper\\\"), where both papers are identified by the \\\"paperId\\\" field. Citations have attributes of their own, (influential classification, intent classification, and citation context).\\n2.4B records in 30 8.5GB files.\",\n",
      "      \"README\": \"Semantic Scholar Academic Graph Datasets\\n\\nThe \\\"citations\\\" dataset provides details about one paper's citation of another paper.\\n\\nSCHEMA\\n - isinfluential: true/false if the citation is considered influential. https://www.semanticscholar.org/faq#influential-citations\\n - contexts: Text surrounding the citation in the source paper's body.\\n - intents: Classification of the intent behind the citations. https://www.semanticscholar.org/faq#citation-intent\\n\\nLICENSE\\nThis collection is licensed under ODC-BY. (https://opendatacommons.org/licenses/by/1.0/)\\n\\nBy downloading this data you acknowledge that you have read and agreed to all the terms in this license.\\n\\nATTRIBUTION\\nWhen using this data in a product or service, or including data in a redistribution, please cite the following paper:\\n\\nBibTex format:\\n@inproceedings{Ammar2018ConstructionOT,\\n  title={Construction of the Literature Graph in Semantic Scholar},\\n  author={Waleed Ammar and Dirk Groeneveld and Chandra Bhagavatula and Iz Beltagy and Miles Crawford and Doug Downey and Jason Dunkelberger and Ahmed Elgohary and Sergey Feldman and Vu A. Ha and Rodney Michael Kinney and Sebastian Kohlmeier and Kyle Lo and Tyler C. Murray and Hsu-Han Ooi and Matthew E. Peters and Joanna L. Power and Sam Skjonsberg and Lucy Lu Wang and Christopher Wilhelm and Zheng Yuan and Madeleine van Zuylen and Oren Etzioni},\\n  booktitle={NAACL},\\n  year={2018}\\n}\\n\\n@inproceedings{cohan-etal-2019-structural,\\n    title = \\\"Structural Scaffolds for Citation Intent Classification in Scientific Publications\\\",\\n    author = \\\"Cohan, Arman  and\\n      Ammar, Waleed  and\\n      van Zuylen, Madeleine  and\\n      Cady, Field\\\",\\n    booktitle = \\\"NAACL\\\",\\n    year = \\\"2019\\\",\\n    url = \\\"https://aclanthology.org/N19-1361\\\",\\n    doi = \\\"10.18653/v1/N19-1361\\\"\\n}\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"embeddings\",\n",
      "      \"description\": \"A dense vector embedding representing the contents of the paper.\\n120M records in 30 28GB files.\",\n",
      "      \"README\": \"Semantic Scholar Academic Graph Datasets\\n\\nThe \\\"embeddings\\\" dataset provides embeddings representing a paper's contents in vector form.\\n\\nThe model is based on the SPECTER model available at https://github.com/allenai/specter. However, the embeddings\\nincluded in this dataset are not compatible with the embeddings produced by the pretrained model from that repo.\\n\\nLICENSE\\nThis software is released under the Apache 2.0 license. (https://www.apache.org/licenses/LICENSE-2.0)\\n\\nBy downloading this data you acknowledge that you have read and agreed to all the terms in this license.\\n\\nATTRIBUTION\\nWhen using this data in a product or service, or including data in a redistribution, please cite the following paper:\\n\\nBibTex format:\\n@inproceedings{specter2020cohan,\\n  title={{SPECTER: Document-level Representation Learning using Citation-informed Transformers}},\\n  author={Arman Cohan and Sergey Feldman and Iz Beltagy and Doug Downey and Daniel S. Weld},\\n  booktitle={ACL},\\n  year={2020}\\n}\\n\\n\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"papers\",\n",
      "      \"description\": \"The core attributes of a paper (title, authors, date, etc.).\\n200M records in 30 1.5GB files.\",\n",
      "      \"README\": \"Semantic Scholar Academic Graph Datasets\\n\\nThe \\\"papers\\\" dataset provides core metadata about papers.\\n\\nSCHEMA\\nSee https://api.semanticscholar.org/api-docs/graph#tag/Paper-Data\\n\\nThis dataset does not contain information about a paper's references or citations.\\nInstead, join with citingPaperId/citedPaperId from the \\\"citations\\\" dataset.\\n\\nLICENSE\\nThis collection is licensed under ODC-BY. (https://opendatacommons.org/licenses/by/1.0/)\\n\\nBy downloading this data you acknowledge that you have read and agreed to all the terms in this license.\\n\\nATTRIBUTION\\nWhen using this data in a product or service, or including data in a redistribution, please cite the following paper:\\n\\nBibTex format:\\n@inproceedings{Ammar2018ConstructionOT,\\n  title={Construction of the Literature Graph in Semantic Scholar},\\n  author={Waleed Ammar and Dirk Groeneveld and Chandra Bhagavatula and Iz Beltagy and Miles Crawford and Doug Downey and Jason Dunkelberger and Ahmed Elgohary and Sergey Feldman and Vu A. Ha and Rodney Michael Kinney and Sebastian Kohlmeier and Kyle Lo and Tyler C. Murray and Hsu-Han Ooi and Matthew E. Peters and Joanna L. Power and Sam Skjonsberg and Lucy Lu Wang and Christopher Wilhelm and Zheng Yuan and Madeleine van Zuylen and Oren Etzioni},\\n  booktitle={NAACL},\\n  year={2018}\\n}\\n\\n\\n\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"s2orc\",\n",
      "      \"description\": \"Full-body paper text parsed from open-access PDFs. Identifies structural elements such as paragraphs, sections, and bibliography entries.\\n35M records in 30 3GB files.\",\n",
      "      \"README\": \"Semantic Scholar Academic Graph Datasets\\n\\nThe \\\"s2orc\\\" dataset contains parsed full-body text from selected papers.\\n\\nA subset of this data was previously released (in a different format) as S2ORC https://github.com/allenai/s2orc\\n\\nThe body text is parsed from PDF documents using Grobid, documented at https://grobid.readthedocs.io.\\nIts output is converted from XML into a single string with a set of annotation spans.\\n\\nSCHEMA\\n - externalIds: IDs of this paper in different catalogs\\n - content:\\n   - source:\\n\\t   - pdfUrls: URLs to the PDF\\n\\t   - oaInfo: license/url/status information from Unpaywall\\n   - text: Full body text as a single string\\n   - annotations: Annotated spans of the full body text\\n\\n\\nLICENSE\\nThis collection is licensed under ODC-BY. (https://opendatacommons.org/licenses/by/1.0/)\\n\\nBy downloading this data you acknowledge that you have read and agreed to all the terms in this license.\\n\\nATTRIBUTION\\nWhen using this data in a product or service, or including data in a redistribution, please cite the following paper:\\n\\n@inproceedings{lo-wang-2020-s2orc,\\n    title = \\\"{S}2{ORC}: The Semantic Scholar Open Research Corpus\\\",\\n    author = \\\"Lo, Kyle  and Wang, Lucy Lu  and Neumann, Mark  and Kinney, Rodney  and Weld, Daniel\\\",\\n    booktitle = \\\"Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics\\\",\\n    month = jul,\\n    year = \\\"2020\\\",\\n    address = \\\"Online\\\",\\n    publisher = \\\"Association for Computational Linguistics\\\",\\n    url = \\\"https://www.aclweb.org/anthology/2020.acl-main.447\\\",\\n    doi = \\\"10.18653/v1/2020.acl-main.447\\\",\\n    pages = \\\"4969--4983\\\"\\n}\\n\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"tldrs\",\n",
      "      \"description\": \"A short natural-language summary of the contents of a paper.\\n58M records in 30 200MB files.\",\n",
      "      \"README\": \"Semantic Scholar Academic Graph Datasets\\n\\nThe \\\"tldrs\\\" dataset provides short natural-language summaries of a paper's content.\\n\\nThe model is based on the SciTLDR model available at https://github.com/allenai/scitldr.\\n\\nLICENSE\\nThis collection is licensed under ODC-BY. (https://opendatacommons.org/licenses/by/1.0/)\\n\\nBy downloading this data you acknowledge that you have read and agreed to all the terms in this license.\\n\\nATTRIBUTION\\nWhen using this data in a product or service, or including data in a redistribution, please cite the following paper:\\n\\nBibTex format:\\n@article{cachola2020tldr,\\n  title={{TLDR}: Extreme Summarization of Scientific Documents},\\n  author={Isabel Cachola and Kyle Lo and Arman Cohan and Daniel S. Weld},\\n  journal={arXiv:2004.15011},\\n  year={2020},\\n}\\n\\n\\n\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://api.semanticscholar.org/datasets/v1/release/\"\n",
    "\n",
    "# Set the release id we will work with the first release\n",
    "release_id = \"2022-05-10\"\n",
    "\n",
    "# Make a request to get datasets available the latest release\n",
    "response = requests.get(base_url + release_id)\n",
    "\n",
    "# Print the response data\n",
    "print(json.dumps(response.json(), indent=2))\n",
    "res=json.loads(response.text)[\"datasets\"]\n",
    "dbs = []\n",
    "for dataset in res:\n",
    "    dbs.append(dataset[\"name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each database is really big and because of that it is divided different downloadable parts. Let's list the first link of each part, click on it to download it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstracts download url:\n",
      "https://ai2-s2ag.s3.amazonaws.com/staging/2022-05-10/abstracts/20220513_070629_00025_mtwkq_0b601c4a-dca2-4eab-921b-1c86bcba0147.gz?AWSAccessKeyId=ASIA5BJLZJPWV4IQXRFZ&Signature=ub6RkmV1DIeT7YctnfZDpGKeMT0%3D&x-amz-security-token=IQoJb3JpZ2luX2VjEMv%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLXdlc3QtMiJHMEUCIH8fAKDF0J3RAmWi%2FFCeSg0zCX1ETGWnHOZ%2BgDTgeWyqAiEA0EvDToJJR2uICdIfo5gc7yScXg%2FyCPi%2Bz3UUeRqCWjwq%2FwMIFBAAGgw4OTYxMjkzODc1MDEiDA9dRDpcG%2FWDtnl1eircA4KFxd3%2BpoQ2jl1Im2eQPtyq20ixJx6kOw1e3lnwU1nSHG0vkIz3TTrSrFfAlW01B%2B82PKmPjARceLzCRIdH811%2FI2STtNiTopP4A02LpSiq1zW4M1OlbHtXq8QknsyKPkWehzPjQHoNSa%2FjPSGC2ymBnZqmC6mIQREln2iXnnoN9qUWvBQIJ03Gvs%2FBJrg1kYciuChnXQqxpVyHGjP%2FkXOBZvW%2FsPHP5q1gB2UOcPvYRMVe2ABF5Tn%2Fiw59Z231aEuH3MjcH5UvWGc4Hy9JF2eqpde7b49dOeHTDDdyFDch28OqMbVg%2BP7m6HYPt%2BZv8u9dgZA%2FflQWmqBGpbzLZD8HH%2FLskBNSvAy5SfxkpF%2BNdx5PvDBUP2ehJBdALxhHq%2BrdE35URlXU8XM8Mga1kogILTRJtqrqo6MozfcFMZvQoVet0WQ%2FlOtHyG2GpthLQ9L4NxMyLZblzj7DSnL8z%2FqGUjicdCMxN9MRyo1Jh8AzQiqzO3uTBRAwzIft4EHuGkgW5TsAVRpzbwEwaVUprCZy3cmdLhtXZ2M8NVc3t3AznwWruE%2B2ifd3BmwCouP6KiJE%2FR8pgjw2NQ9fYT6PqnIn69LTeD42y5dTyKB5AfFlxJQjhhKIWsx%2BzSABMJLPoL4GOqUBM9PKg9rEUdwZRwihppTdcKKcqCPzq7uf6%2FQ3MImwi7uXbmgNZTgKVBgaqUNfE%2B3okD1QdTnP%2BWZMuOJP71QpZh9VLZc9hM9kgdaQHEcv8W0vPUtUicY8DugL4xfTYHR3dNdcfyfZBXovAJ3PAU4LOziSMWhYCr0ixg6zfOy6ir4vdjXhhKgda2cu4R3UkJRA9XItL5QsLkgs9PyEXNKMgPl66cAz&Expires=1741790317\n",
      "authors download url:\n",
      "https://ai2-s2ag.s3.amazonaws.com/staging/2022-05-10/authors/20220513_071222_00034_g4b97_046945fe-8bc5-4e7b-8bca-95cc504838e8.gz?AWSAccessKeyId=ASIA5BJLZJPWXYEBHI72&Signature=wJiCUnQh2Eoa0HzLs1%2BTpa9XtgM%3D&x-amz-security-token=IQoJb3JpZ2luX2VjEM7%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLXdlc3QtMiJHMEUCIQD0ArBpN8GVvDeZKUHnz%2BGc1%2B4yLlSmy5eoGEFsh1euwQIgY0cq78Z4BeZQ0Cy1XIAOlVW%2B0%2BrEu3KldhMJ43W9hZIq%2FwMIFxAAGgw4OTYxMjkzODc1MDEiDDmPSj7otk3TV8gRUyrcA7mLfifxs%2BHoXQxKXh3DmHlXj8AfLGRrX55jZOotTBZYAkPffYnGHVCr8TFN2wW1iHdYlrkeXah8yrFBthkVV9I8qYFpizsYng2ngjew6AwfOHuqVlNpNPjDAFt48%2BC7wf4oLa4hUWUKJx59ob3vn8%2FDjdjve0S%2FWE9W1PY75qMCqdOpk%2B7VKDgY5wyG40%2FglP3xYdOXMUI7HgejKJ%2BhdmkjgFY3vPMRTbvHGVmEvkE9%2BjcT8hp9p1cL%2BOplQbt%2F%2F1YgHrKwNOOSYOSz8EF42bBanMBQXu7TQMQf09xQHg0tHCZbE%2Bk1pl01FA8G9b1VQZ48CGglJ2bg%2FzpIliTUPf7%2B5PEpaNMhCvpXmWls%2FgeassL%2FcPhWHVunKpOlVGhw9LBaEyqg%2BlgpXhCaNyviGnjNAGaJbWXxM3RIrFvEWJfBb1emvrh3NmHvTnIME6na1cmPc1geoquUOOmFh20FCSCReBsRTg%2F5JmAaugbeweKWelHu%2FjLRv6M1A32y0fo7iLaNXJdMFxCq2QmdieyPrMomTEV5YUwZfbdS6LdCUmA8Z%2Blfxa3X%2B5Ne1Md09uK8sGHjvJBEM9vVwPknAlUDOXil%2FL4O2sYrqxp9N8Ern41zn8Zp2QI%2B1FRAneOoMKiiob4GOqUBPj0AvHMXk%2B%2F2hjvJNCgW7MT8sgsZdxgmbRCTYX7xVai81mEGBIAMegoipkybdnHVCjqa41ImAYnCn1NkHFVtbw24TJVT0wMi2QeOozLQ%2Bao1GkBSPM9v68GTeVaEnV5LTYP6Mu3iP1Lsm%2BqN6dwUIYEUgr4p2PE%2FCiwi6xwen3%2FstH2Weoo%2BxMr2Kd7ThWXlSNYf7mdcBbDJYCqn6C6eawnE0A7t&Expires=1741790318\n",
      "citations download url:\n",
      "https://ai2-s2ag.s3.amazonaws.com/staging/2022-05-10/citations/20220513_071330_00037_xkiix_00bc87b3-00b5-4912-91e0-2e7ea0a433a0.gz?AWSAccessKeyId=ASIA5BJLZJPWXRBQBYVN&Signature=bS%2FIs%2Ba77hJ5JXXwdjJ2ft3dKQM%3D&x-amz-security-token=IQoJb3JpZ2luX2VjEMz%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLXdlc3QtMiJHMEUCIQDreoiBpwkZv63mDlR1rdTPPsQ2U1gS55HrlTtcGLFcMgIgaWEazsatwlCDoN4LWL2Cr80hmf34E8h92tXHYIaKWxQq%2FwMIFRAAGgw4OTYxMjkzODc1MDEiDJBAP4ouRyJwwcUANircA0WIziDmeyGupH3UOa5LEiOk53qlKXMY8VFyhB5yIXw8FnIxSMwtKPTHXnwssXXEUq6yItOlbNAHXFKAU6CKyflmoJwtPmLfpN%2FO1PZcbefLtV6w%2F0aTsShM7IxJ%2BoB6fNFy4FfEaUvB%2FHj3GMFapY4r3PzivhwT%2Fn0ACvhRfqN%2B0v99R6fF6oYQXmCRN6c%2FHzwuimnee1BAJwP6RELOMTLuAjTrdWzJWDS31Cm9%2FqgwJov9dS9GY6gLVoM9ppxkmSM2EHQDP82gX7TSW2EpS%2Fan1ZvlDgBoypDzAmzF6pjuB7aCJAHW6LI%2F%2FRLDt9uHsHJdfJLUNwVzhnLgc5GAY6Qis8MPo38xpsW%2B1LVLsK0GlnR4sXWoUs%2BmFuLZ3jK52EnEquDPJ78QWwSSDqhEIYfGZhXfHLsIlUDVL4lW19%2FDignDk9m4Fj%2F198ZXUYGIFvQfFsz1bfDCmB0HrmyaoK7kA0O9ZctrmjyASXwORwOx0ZIBkFxdgM6%2Fjp8lNURUcireo1orUnDQR6GjG368bKiU%2BvhhgPNiWqlxw7ibbyATiy4BoygkD2ZHbBvJO%2Fx2NPC%2FcGxOPHCSGOnqrkPmg7Yz%2Bs5KX%2BZOSTytGHKVy%2BCTbFtEURsSAvmVR3qrMPf2oL4GOqUB5QPSNpo9If7G3o6HhlhXwSFYQh27eMDMAW4eZPRHyTZV8oQpMOEpIdqc82bIVlRougnjLo0JLPRXmxGaT4MGV98PqjaT%2BYurJrN%2BUnLLR8ti1UJdBqy%2FTj5CMQvqgBC2boXeuWm09puPG9eF9GCZ15EZShG0p7vhWiyf0Hs0dqAWIebEj%2FfhHX%2BYa0ipqIkEazEDzBKoYL%2BpHQIg9sz%2F65RUgDt8&Expires=1741790320\n",
      "embeddings download url:\n",
      "https://ai2-s2ag.s3.amazonaws.com/staging/2022-05-10/embeddings/20220513_073500_00025_jcu4c_08b8baf2-dfe2-4d95-804a-01e951f4b532.gz?AWSAccessKeyId=ASIA5BJLZJPWWX7UMFOX&Signature=CkL6sKQTCZ8w%2F%2FYuviZ32EB3aMk%3D&x-amz-security-token=IQoJb3JpZ2luX2VjEM7%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLXdlc3QtMiJIMEYCIQCodCsH1RuiaFsLM%2BylH%2FpECT6kTIGaotTw%2BJ%2FkOgrmFAIhAOS1XRs5RyK9jmDflAX8IoIiPDsUosZu8NyYj7u3c7GDKv8DCBcQABoMODk2MTI5Mzg3NTAxIgxasMfYxTqDawrtAO0q3AOUfAjEyiX8aNyRjU8MlMpSahi6X%2FwcabH8KvNBvUBFaQsD2b%2BT48AKBSm67dcRYRVeyPCe2qLIn6uU3PasyjS9sLBakBeftGo%2BEH2y5XsXdjUv7fcImPV8lkR9MlP7SR0dzPk6l6KH9k4oHQq1iM%2Fv2uXkerT%2FH%2FxwEo%2FBsvK3nWwVoGtC%2BL18pQVNR6ijiEHG%2BH%2Fhy8VpjYdQEUc43BFgfgeX7bdd5nL4KwXSKBUmkUFvewZXSRk2%2Bz4e%2FOqZAb5uAFCAC%2BdqKehlfwHOC1U1nK4jAHgAxXBe6icGa%2B8llCOOth5YiT3bQDPUpVOm3rsZrkfGbgmpV4kwh%2BxGcdIoweo7v5jC2k51P9Kjpb2lI3OST39zd3jsMZVRFWZ7r8x8fPSWsMCEKyrS8wvSrsBC%2FxROmCBYLKjxuMmTd8POvjSUkzQJS7nJOe%2FWz6XWrONmy%2BaUdq1A8p4h9FauDDgdQCI50zGgxwfJ9gzgt2yWz%2FIJ1BbrIy93S%2Fiow74sjTWPOMiD%2B9h4oK9oXIhTScmoj4Oz%2BBGrqLrPsnpIp2JjOvpqWs%2Bd0L4kkveaeOYDUaSOsoDMLp6zdCrQmL3hQRoLe3KBExLu66voK9McVjSKmTy%2F5PFydKbIjRw7STDurKG%2BBjqkAQkMUXNc3Y3JqHxOrPzQvSMMZ26LCj7GfNs00dxd%2FiqMY88mkDfrRblJSD%2BHgwC6tyeV8PmcSyUw%2FVp43qJ4RjbKWlr7lSqC5e5ZN8PFV2pb%2BGzTSShDDH6KFvL4%2BDDd9akPFXQcEcLHelhrlamKSCmbV3OJtR9v2uG2gbLF5QA2vT6qho7UAeVPU9aqciDFq75x6xSL17uvaKC8qA7KXi0Bf3YA&Expires=1741790321\n",
      "papers download url:\n",
      "https://ai2-s2ag.s3.amazonaws.com/staging/2022-05-10/papers/20220513_070151_00012_4bpee_054cd248-48ad-47d6-b0d9-e4fe4333cc61.gz?AWSAccessKeyId=ASIA5BJLZJPWRCVT7POI&Signature=odFWgPm9tQd1gjGqCUFGZk%2Bra%2BE%3D&x-amz-security-token=IQoJb3JpZ2luX2VjEMz%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLXdlc3QtMiJHMEUCIBmS%2FfT4zgQo2oeRv60nFL0TsAi88zT%2BF3L2OPGuFIbEAiEAwIet41mtyFlTR16dOdJxzrZRKRnhce3jlk7r8r1yP7Uq%2FwMIFRAAGgw4OTYxMjkzODc1MDEiDAWVmhIW0KBE2BsDtyrcAwVIHWR9EZz7M%2BcWc4YpNU2PI3Q8fIeV7sSe0Heh5uf64qsbke0R1Q0p5Ifp2BKt7jCTgBQcu2Tfy46Sp54okum83GEbUnuiLGAPJhvG%2FribSjtNKa3pG1m4TJiBs04WO8BMVBRr6jc8iuGcQ%2B9nOynRNL%2FVu0IUQjoUbMWkhicruF1BzXsLpv8cnVR8DXE04Xpj%2B%2Fen3FcTmNwPeb9WvyT6Zmz91r3GfzjJqhBAFKF%2BhWZoffQwCzIJq%2FpKERXH0zaeuuSDu7hkI8GYSbyN27QLTcpYnigEXwqQiK7Nf5aoz%2BrLBBqd6Wzeu5Owj1nPg7Hr53RDCi2qkbEqVChb9LN1TTHYt4grWzm2jgW%2BhOWVWTW%2BoJSx2HYI7kclmuQAT58gyRUoL03Uh0ykjZugOoTQsiJKad5HW2GupVRFb3qKRtXhYKWx0vSiOFclbkGEixfTVpHck0%2BE%2BBFcPlzplUdA83qCCL16T48UYCxiUtcFBVpRG%2FSiHOdx0pXL%2BN5oQ8NxxgEgkeTawZjpx1eghqM6GW4XJK%2FmmT3j%2BPGGH%2Bm5RydRYm2LF6ubLh0MJfDVyqCXEo7qvX%2BT8QHRte0l58of2gmJj4qX5NmdVbBM8cGp%2BkOY03T9vUFpZyQzMOrtoL4GOqUBXCTioy7TGjhSxkrK9K6jaGMwZzNNDZ2msYPPtxyyMJw3bwVAL0xdgt3LBTMGe9YhrnBHmPxORUm5sklH8xY4oDh186426d4wEwubihurIcdCRfPFsNBxYBo%2FWds4IWaivtaKvZIeBZyrVoOTuJBpEdZWxCn1eNgAKjfvrR7cVHnQ2R7K7jPn%2FDZTvq7NWgcU2AJ2%2FFRum6SXXEOLs%2Bn7r8shk3gN&Expires=1741790322\n",
      "s2orc download url:\n",
      "https://ai2-s2ag.s3.amazonaws.com/staging/2022-05-10/s2orc/20220519_000932_00025_dauyy_04c9ef07-dd83-42f4-b36a-4d3ea06c99b0.gz?AWSAccessKeyId=ASIA5BJLZJPWSRMIY22M&Signature=dTLx7VN9FNnJFMUN78203i19izo%3D&x-amz-security-token=IQoJb3JpZ2luX2VjEMz%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLXdlc3QtMiJGMEQCIH3CoGeeaIJBMSSQYS05ZVEHXIbjtRSssis6WqL2G%2BG3AiBGUbdLOzlESQpfOzczEmGYozYLH%2BYfmCmF%2FkIS7C2Njyr%2FAwgVEAAaDDg5NjEyOTM4NzUwMSIMZqjWl8naLR7L8PWrKtwDtojsgP3MCxB2IBriHgX1F8yM87l6Lq9SoYHKiiHvJMEwLJzHzei%2FmDkXXL%2Fxm3fnjQ7u9LxGD%2F7cpU3CmrjGfYPPhFza2gy8dLs5YcHfUBovKb3cbO7sO6hkkAbDmlJnFYezqPcLYHNtPAPJZaj6jOLpEgl9r%2F7tgPDjRQ6MxVfG53iLHOdKLX0pki%2FMJaJ%2BGEXK8vMKnywyhAZcRWeYPuZiR4EXtHzRvI7AS4rd2mhfzL2dirajeJZipEcfQ3N6r2r29cyH1jdSrnw65h5NnoW4IYfE5JzXDL5IM19l9LywkSv94Wh9disDMZuxJM4TzYqoTuzitcDTkh7T63e2H4N%2Fb8uclTp3IPPOraTvH%2FCqBeGpVCdVgQ1CbYxxuapE%2Fv3sMMThRfYXTZUNFEr2iVQQ0WoO7jikhDpyZeG3ARE6ZNxwKck4gB43ZEIWRTYyZg8V2IV1RSz59Qi4PmioTXvfKwu32yR2hlW5kwE9Yd1U4vtO8SyOXukMpaZhZ3Mvpesgz7GAMtmcS%2FabofaMMH0bIX%2BigpHBlPM174EtjhGb%2F7i9J95gNk%2Bei2aAug5L6TUVRgYW9yk%2FRMv3VeCH55YRJ%2FiICK%2F7CQwRC1PZKwISRdPzcpnTAhll3o0w5vugvgY6pgG3TV3FyLYDYGt2jNd5NhpiEy280cZ06JadKWE45ZuVYvs2surrkSIgCp%2B0vFIS%2BZs9%2Bi7yxW0SX0r%2Bx0yqW5y6zTNDSgJRSokzknh1%2Bz3kHiYsOxaVNjAqasa2Eu5uiSuPu9ie5dMg1hCo2Ohxs%2B3WbbYTs%2ByuiQvfgZsh%2FTRYZX9uyCP5hRyJycn%2FGmDmIRGd3nHCT%2BC%2BMlsnWMeJzKhL0SxQPCgU&Expires=1741790323\n",
      "tldrs download url:\n",
      "https://ai2-s2ag.s3.amazonaws.com/staging/2022-05-10/tldrs/20220513_073419_00109_m3j5m_02586bff-4a75-46c4-8f1f-e2d8ad0bbf75.gz?AWSAccessKeyId=ASIA5BJLZJPWQH7YB6SZ&Signature=ygGg3lDR7hgkSBXpwYmnr%2F1T97c%3D&x-amz-security-token=IQoJb3JpZ2luX2VjEM3%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLXdlc3QtMiJHMEUCICcmlNYU8AmokiDwy4dQpdLUxiwuNEo%2BlvVwEgdKhFa%2BAiEAl7fFZSw86JE0YKiORVFmH6lHtLJGmHlxccVIuzX%2F1Ocq%2FwMIFhAAGgw4OTYxMjkzODc1MDEiDKlQJf%2BsK%2FYkvUKqAyrcA83EEJQeZ%2FOZtv2C%2BAxfYw8l1SgexlDsK5LJQmCc3Yll5mEO7KHIxA%2B89uzH8QrFZQnA%2FSP0VLm1nfD39WY18fNCN68%2BopRjZkVMoqLca82Jgww5821QSgDhMIY9fRnTp%2FQirnQxd0IVzFtrojHgsFADlydytZ9LjDwOYXTw6q7zY0FFXQRTn%2FGAOYvpM%2BHiO2LPh9J22ycDCay%2Fi%2FCODxVF0KCjOsl0jPfa4XXmGfwwO26Jyqslfuo7yW58kdjq3wyFBG3heiAKwQyO9sxQYmpkioputPUfaK8FfngkBcMAnmHJPT8ey9A%2BXGcHRiSUcZMH%2BPRkcoLElKRaTUQdu6cCAu%2FX6nATc2GDcDxAbhwHQh2Dvm7wo2fhpPjWyvEPGZhNVPdaOd9QM1x%2BcxPk2au9TJGuj2HqZduBiaoygHT3hVQ%2FtbvmBWLNawaaD9YZkfm9weUzFzm4DYBpnVdHvgnWX9368RLv6k2XGEQT%2FT2MNJbxdPm5Z0sA1g8gdsAVTM9zmBw%2FPylyJePw5AuPnBHjC5lMGB9WlXOWZhChOZS5FE%2BYDi1v6cpkSrFmhmySv8gE2O8DRB%2BI6hRUhPB9XqeEpby82AB6wkoJum3jmRQpVuaNA5%2FPZMHSoDTJMMmFob4GOqUBPOCyY7Fw%2B4k98p89zJvAl%2BIKRKW5nnTBHqjGqgtsSBOEv%2FJy1y4O83kbzBSx0anAf4px6aPhxU3J%2BA0aR5omnXjrhB%2BMp1d5R8Cjw%2BVzsWfitEnmmz3YCz12BZ%2Fd9aBwevyCrdyhmc4o7JAPzkcGBobJyIXi4u83eIWl%2FCjzN4SjAis6uE92GL8krdayydVmvvzaWSpEJW%2BC9XqoXHNiSZUsqNx8&Expires=1741790324\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "# This endpoint requires authentication via api key\n",
    "api_key = os.environ.get('API_KEY')\n",
    "headers = {\"x-api-key\": api_key}\n",
    "\n",
    "for name in dbs:\n",
    "    # Define dataset name you want to download\n",
    "    dataset_name = name\n",
    "    # Send the GET request and store the response in a variable\n",
    "    response = requests.get(base_url + release_id + '/dataset/' + dataset_name, headers=headers)\n",
    "    print(name+\" download url:\")\n",
    "    url=json.loads(response.text)[\"files\"][0]\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put each json file in a folder called rawdata and change their names to the appropiate ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will start cleaning the data, we will start with the papers and keywords. First let's define the conditions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the condition for deleting a row\n",
    "def conference_conditions(json_obj):\n",
    "    if json_obj[\"authors\"] == []:\n",
    "        return True\n",
    "    elif json_obj[\"venue\"] == None:\n",
    "        return True\n",
    "    elif json_obj[\"s2FieldsOfStudy\"] == None:\n",
    "        return True\n",
    "\n",
    "def journal_conditions(json_obj):\n",
    "    if json_obj[\"authors\"] == []:\n",
    "        return True \n",
    "    elif json_obj[\"journal\"] == None or \"name\" not in json_obj[\"journal\"] or \"pages\" not in json_obj[\"journal\"] or \"volumen\" not in json_obj[\"journal\"]:\n",
    "        return True\n",
    "    elif json_obj[\"s2FieldsOfStudy\"] == None:\n",
    "        return True\n",
    "    \n",
    "def save_to_csv(json_obj, filename):\n",
    "\n",
    "                  \n",
    "                \n",
    "                row_papers = {\n",
    "                \"sid\": count, # Add a new column with a surrogated ID, just in case\n",
    "                \"corpusid\": json_line.get(\"corpusid\"),\n",
    "                \"title\":  json_line.get(\"title\").strip().replace(\"\\n\", \" \"),\n",
    "                \"authorId\": json_line.get(\"authorId\"),\n",
    "                \"authorName\": json_line.get(\"authorName\"),\n",
    "                \"url\": json_line.get(\"url\"),\n",
    "                \"year\": json_line.get(\"year\"),\n",
    "                \"referencecount\": json_line.get(\"referencecount\"),\n",
    "                \"citationcount\": json_line.get(\"citationcount\"),\n",
    "                \"influentialcitationcount\": json_line.get(\"influentialcitationcount\"),\n",
    "                \"publicationtype\": json_line.get(\"publicationtypes\"),\n",
    "                \"publicationdate\": json_line.get(\"publicationdate\"),\n",
    "                \"venue\": json_line.get(\"venue\", ''),\n",
    "                \"publicationvenueid\": json_line.get(\"publicationvenueid\", ''),\n",
    "                \"journalName\": json_line.get(\"journalName\"),\n",
    "                \"journalVolume\": json_line.get(\"journalVolume\"),\n",
    "                \"journalPages\": json_line.get(\"journalPages\")\n",
    "                }\n",
    "                # Write the row to CSV 1\n",
    "                csv_writer_1.writerow(row_papers)\n",
    "\n",
    "                keywords=list()\n",
    "                for keyword in json_line.get(\"s2fieldsofstudy\", []):\n",
    "                    keywords.append(keyword[\"category\"])\n",
    "                keywords = list(set(keywords))\n",
    "                for keyword in keywords:\n",
    "                    row_keywords = {\n",
    "                        \"sid\": count, # We will use the surrogated ID here, just in case\n",
    "                        \"keyword\": keyword\n",
    "                    }\n",
    "                    # Write the row to CSV 2 for each keyword\n",
    "                    csv_writer_2.writerow(row_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified JSONL saved to csv/papers.csv\n"
     ]
    }
   ],
   "source": [
    "output_file = \"csv/papers.csv\"  \n",
    "keywords_file=\"csv/keywords.csv\"\n",
    "\n",
    "api_key = os.environ.get('API_KEY')\n",
    "headers = {\"x-api-key\": api_key}\n",
    "\n",
    "#********************************************************************************************************************\n",
    "RECORDS = 50  # Number of records to save per cathegory \n",
    "PUBLICATION_TYPES = [\"Conference\", \"JournalArticle\"]  # Publication types to filter (There is not Workshop)\n",
    "QUERY = \"data\"  # Query to filter the papers\n",
    "FIELDS = \"paperId,corpusId,title,abstract,authors,url,year,referenceCount,citationCount,influentialCitationCount,s2FieldsOfStudy,publicationDate,journal,venue,publicationVenue\"  # Fields to retrieve from the API\n",
    "#********************************************************************************************************************\n",
    "\n",
    "query_encoded = urllib.parse.quote(QUERY)\n",
    "fields_encoded = urllib.parse.quote(FIELDS)\n",
    "\n",
    "count = 0\n",
    "\n",
    "with open(output_file, \"w\", newline='', encoding=\"utf-8\") as outfile  :   \n",
    "    csv_writer_1 = csv.DictWriter(outfile, fieldnames=[\"sid\",\"paperId\",\"corpusId\", \"title\", \"authorId\", \"authorName\", \"url\", \"year\", \"referenceCount\", \"citationCount\", \"influentialCitationCount\",\"publicationType\", \"publicationDate\"])\n",
    "\n",
    "    # Write the headers to the CSV files\n",
    "    csv_writer_1.writeheader()\n",
    "    for publication_type in PUBLICATION_TYPES:\n",
    "        type_encoded = urllib.parse.quote(publication_type)\n",
    "        url1=\"https://api.semanticscholar.org/graph/v1/paper/search?query=\"+query_encoded+\"&publicationTypes=\"+type_encoded+\"&fields=\"+fields_encoded+\"&limit=\"+str(RECORDS)\n",
    "        response = requests.get(url1, headers=headers).json()\n",
    "        for line in response[\"data\"]:\n",
    "            count += 1\n",
    "            try:  \n",
    "                if publication_type==\"Conference\":\n",
    "                    if not conference_conditions(line):\n",
    "                        line[\"publicationType\"]=\"Conference\"\n",
    "                        line[\"journalName\"]=None\n",
    "                        line[\"journalVolume\"]=None\n",
    "                        line[\"journalPages\"]=None\n",
    "                elif publication_type==\"JournalArticle\":\n",
    "                    if not journal_conditions(line):\n",
    "                        line[\"publicationType\"]=\"JournalArticle\"\n",
    "                        line[\"venue\"]=None\n",
    "                        line[\"journalName\"]=line[\"journal\"][\"name\"]\n",
    "                        line[\"journalVolume\"]=line[\"journal\"][\"volume\"]\n",
    "                        if line[\"journal\"][\"pages\"] is not None:\n",
    "                            line[\"journalPages\"]=re.sub(r'\\s+', '', line[\"journal\"][\"pages\"])\n",
    "                        else:\n",
    "                            line[\"journalPages\"]=None    \n",
    "                line[\"authorId\"] = line[\"authors\"][0][\"authorId\"]\n",
    "                line[\"authorName\"] = line[\"authors\"][0][\"name\"]\n",
    "                row_papers = {\n",
    "                    \"sid\": count, # Add a new column with a surrogated ID, just in case\n",
    "                    \"paperId\": line.get(\"paperId\"),\n",
    "                    \"corpusId\": line.get(\"corpusId\"),\n",
    "                    \"title\":  line.get(\"title\").strip().replace(\"\\n\", \" \"),\n",
    "                    \"authorId\": line.get(\"authorId\"),\n",
    "                    \"authorName\": line.get(\"authorName\"),\n",
    "                    \"url\": line.get(\"url\"),\n",
    "                    \"year\": line.get(\"year\"),\n",
    "                    \"referenceCount\": line.get(\"referenceCount\"),\n",
    "                    \"citationCount\": line.get(\"citationCount\"),\n",
    "                    \"influentialCitationCount\": line.get(\"influentialCitationCount\"),\n",
    "                    \"publicationType\": line.get(\"publicationType\"),\n",
    "                    \"publicationDate\": line.get(\"publicationDate\"),\n",
    "                }\n",
    "                csv_writer_1.writerow(row_papers)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON: {e}\")\n",
    "print(f\"Modified JSONL saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.semanticscholar.org/graph/v1/6432563/citations\n",
      "{\n",
      "  \"error\": \"Not found\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfor id in ids: \\n    url = \"https://api.semanticscholar.org/graph/v1/\"+str(id)+\"/citations\"\\n    print(url)\\n\\n    response = requests.get(url, headers=headers).json()\\n    print(json.dumps(response, indent=2))\\n# Save the results to json file\\n\\nwith open(output_file, \"w\", newline=\\'\\', encoding=\"utf-8\") as outfile  :   \\n    csv_writer_1 = csv.DictWriter(outfile, fieldnames=[\"sid\",\"authorId\", \"url\", \"name\", \"paperCount\", \"hIndex\"])\\n\\n    # Write the headers to the CSV files\\n    csv_writer_1.writeheader()\\n    for line in response:\\n        count+=1\\n        try:  \\n            row_papers = {\\n                        \"sid\": count, # Add a new column with a surrogated ID, just in case\\n                        \"authorId\": line.get(\"authorId\"),\\n                        \"url\": line.get(\"url\"),\\n                        \"name\": line.get(\"name\"),\\n                        \"paperCount\": line.get(\"paperCount\"),\\n                        \"hIndex\": line.get(\"hIndex\")\\n                        }\\n            # Write the row to CSV 1\\n            csv_writer_1.writerow(row_papers)\\n        except json.JSONDecodeError as e:\\n            print(f\"Error decoding JSON: {e}\")\\nprint(f\"Modified JSONL saved to {output_file}\")\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_file=\"csv/citations.csv\"\n",
    "papers_file = \"csv/papers.csv\"  \n",
    "\n",
    "\n",
    "df=pd.read_csv(papers_file)\n",
    "ids=df[\"corpusid\"].values.tolist()\n",
    "\n",
    "api_key = os.environ.get('API_KEY')\n",
    "headers = {\"x-api-key\": api_key}\n",
    "\n",
    "url = \"https://api.semanticscholar.org/graph/v1/6432563/citations\"\n",
    "print(url)\n",
    "\n",
    "response = requests.get(url, headers=headers).json()\n",
    "print(json.dumps(response, indent=2))\n",
    "\n",
    "\"\"\"\n",
    "for id in ids: \n",
    "    url = \"https://api.semanticscholar.org/graph/v1/\"+str(id)+\"/citations\"\n",
    "    print(url)\n",
    "\n",
    "    response = requests.get(url, headers=headers).json()\n",
    "    print(json.dumps(response, indent=2))\n",
    "# Save the results to json file\n",
    "\n",
    "with open(output_file, \"w\", newline='', encoding=\"utf-8\") as outfile  :   \n",
    "    csv_writer_1 = csv.DictWriter(outfile, fieldnames=[\"sid\",\"authorId\", \"url\", \"name\", \"paperCount\", \"hIndex\"])\n",
    "\n",
    "    # Write the headers to the CSV files\n",
    "    csv_writer_1.writeheader()\n",
    "    for line in response:\n",
    "        count+=1\n",
    "        try:  \n",
    "            row_papers = {\n",
    "                        \"sid\": count, # Add a new column with a surrogated ID, just in case\n",
    "                        \"authorId\": line.get(\"authorId\"),\n",
    "                        \"url\": line.get(\"url\"),\n",
    "                        \"name\": line.get(\"name\"),\n",
    "                        \"paperCount\": line.get(\"paperCount\"),\n",
    "                        \"hIndex\": line.get(\"hIndex\")\n",
    "                        }\n",
    "            # Write the row to CSV 1\n",
    "            csv_writer_1.writerow(row_papers)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON: {e}\")\n",
    "print(f\"Modified JSONL saved to {output_file}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\requests\\models.py:974\u001b[0m, in \u001b[0;36mResponse.json\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 974\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m complexjson\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    975\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[0;32m    977\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_decoder\u001b[38;5;241m.\u001b[39mdecode(s)\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_decode(s, idx\u001b[38;5;241m=\u001b[39m_w(s, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mend())\n\u001b[0;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\json\\decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx-api-key\u001b[39m\u001b[38;5;124m\"\u001b[39m: api_key}\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Send the API request\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url, params\u001b[38;5;241m=\u001b[39mquery_params, json\u001b[38;5;241m=\u001b[39mdata, headers\u001b[38;5;241m=\u001b[39mheaders)\u001b[38;5;241m.\u001b[39mjson()\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Save the results to json file\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m outfile  :   \n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\requests\\models.py:978\u001b[0m, in \u001b[0;36mResponse.json\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    974\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m complexjson\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    975\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[0;32m    977\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n\u001b[1;32m--> 978\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RequestsJSONDecodeError(e\u001b[38;5;241m.\u001b[39mmsg, e\u001b[38;5;241m.\u001b[39mdoc, e\u001b[38;5;241m.\u001b[39mpos)\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "url = \"https://api.semanticscholar.org/graph/v1/author/batch\"\n",
    "output_file=\"csv/authors.csv\"\n",
    "papers_file = \"csv/papers.csv\" \n",
    "count=0  \n",
    "query_params = {\n",
    "    \"fields\": \"name,url,paperCount,hIndex\"#,papers\"\n",
    "}\n",
    "\n",
    "df=pd.read_csv(papers_file)\n",
    "ids=df[\"authorId\"].values.tolist()\n",
    "\n",
    "data = {\n",
    "    \"ids\": ids\n",
    "}\n",
    "api_key = os.environ.get('API_KEY')\n",
    "headers = {\"x-api-key\": api_key}\n",
    "\n",
    "# Send the API request\n",
    "response = requests.post(url, params=query_params, json=data, headers=headers).json()\n",
    "# Save the results to json file\n",
    "with open(output_file, \"w\", newline='', encoding=\"utf-8\") as outfile  :   \n",
    "    csv_writer_1 = csv.DictWriter(outfile, fieldnames=[\"sid\",\"authorId\", \"url\", \"name\", \"paperCount\", \"hIndex\"])\n",
    "\n",
    "    # Write the headers to the CSV files\n",
    "    csv_writer_1.writeheader()\n",
    "    for line in response:\n",
    "        count+=1\n",
    "        try:  \n",
    "            row_papers = {\n",
    "                        \"sid\": count, # Add a new column with a surrogated ID, just in case\n",
    "                        \"authorId\": line.get(\"authorId\"),\n",
    "                        \"url\": line.get(\"url\"),\n",
    "                        \"name\": line.get(\"name\"),\n",
    "                        \"paperCount\": line.get(\"paperCount\"),\n",
    "                        \"hIndex\": line.get(\"hIndex\")\n",
    "                        }\n",
    "            # Write the row to CSV 1\n",
    "            csv_writer_1.writerow(row_papers)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON: {e}\")\n",
    "print(f\"Modified JSONL saved to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
